<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Simple diagrams of convoluted neural networks - Piotr Migdał</title><meta name="gridsome:hash" content="9a1d7cc863b6d13c1972881243213fa0c697d37b"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" property="og:title" content="Simple diagrams of convoluted neural networks"><meta data-vue-tag="ssr" name="twitter:title" content="Simple diagrams of convoluted neural networks"><meta data-vue-tag="ssr" name="description" content="A good diagram is worth a thousand equations - let&#x27;s create more of these!"><meta data-vue-tag="ssr" property="og:description" content="A good diagram is worth a thousand equations - let&#x27;s create more of these!"><meta data-vue-tag="ssr" name="twitter:description" content="A good diagram is worth a thousand equations - let&#x27;s create more of these!"><meta data-vue-tag="ssr" name="url" content="https://p.migdal.pl/blog/2018/09/simple-diagrams-deep-learning/"><meta data-vue-tag="ssr" property="og:url" content="https://p.migdal.pl/blog/2018/09/simple-diagrams-deep-learning/"><meta data-vue-tag="ssr" name="twitter:url" content="https://p.migdal.pl/blog/2018/09/simple-diagrams-deep-learning/"><meta data-vue-tag="ssr" property="og:image" content="https://p.migdal.pl/assets/img/piotr-migdal-direct-smiling-2022-by-cytacka-thumbnail.c6cd93cf.jpg"><meta data-vue-tag="ssr" name="twitter:image" content="https://p.migdal.pl/assets/img/piotr-migdal-direct-smiling-2022-by-cytacka-thumbnail.c6cd93cf.jpg"><meta data-vue-tag="ssr" name="author" content="Piotr Migdał"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/assets/static/favicon.62d22cb.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/assets/static/favicon.1539b60.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/assets/static/favicon.dc0cdc5.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/assets/static/favicon.7b22250.9bb7ffafafc09ac851d81afb65b8ef59.png"><link rel="preload" href="/assets/css/0.styles.5a6934ab.css" as="style"><link rel="preload" href="/assets/js/app.73194795.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--blog-post-vue.1cb9f861.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.c98891ed.js"><link rel="prefetch" href="/assets/js/page--src--pages--blog-vue.9d0bda9b.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.a35db996.js"><link rel="prefetch" href="/assets/js/page--src--pages--projects-vue.ccfd961b.js"><link rel="prefetch" href="/assets/js/page--src--pages--publications-vue.424fc9cb.js"><link rel="prefetch" href="/assets/js/page--src--pages--resume-vue.5e839003.js"><link rel="prefetch" href="/assets/js/vendors~page--src--pages--blog-vue.e3213408.js"><link rel="stylesheet" href="/assets/css/0.styles.5a6934ab.css"><script data-vue-tag="ssr" src="https://plausible.io/js/plausible.outbound-links.js" async defer data-domain="p.migdal.pl"></script><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="layout"><header class="header"><strong><a href="/" class="active">Piotr Migdał</a></strong><nav class="nav"><a href="/blog" class="nav__link active">Blog</a><a href="/projects" class="nav__link">Projects</a><a href="/publications" class="nav__link">Publications</a><a href="/resume" class="nav__link">Resume</a></nav></header><div class="markdown-header"><h1>Simple diagrams of convoluted neural networks</h1><p class="header-information">
      15 September 2018 | by Piotr Migdał
      <span>
         | <a href="https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b">orginally posted at Medium</a></span>
      | 15 min read
    <!----></p></div><div class="markdown"><blockquote>
<p>A good diagram is worth a thousand equations — let’s create more of these!</p>
</blockquote>
<p>Neural networks are complicated, multidimensional, nonlinear array operations. How can we present a deep learning model architecture in a way that shows key features, while avoiding being too complex or repetitive? How can we present them in a way that is clear, didactic and insightful? (Bonus points if it is beautiful as well!). Right now, there is no standard for plots — neither for research nor didactic projects. Let me take you through an overview of tools and techniques for visualizing whole networks and particular blocks!</p>
<h1 id="the-baseline"><a href="#the-baseline" aria-hidden="true"><span class="icon icon-link"></span></a>The baseline</h1>
<p>AlexNet was a breakthrough architecture, setting convolutional networks (CNNs) as the leading machine learning algorithm for large image classification. The paper introducing AlexNet presents an excellent diagram — but something is missing…</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1964/1*nrNJXwfudc36wWw5Jbmfsg.png" alt="Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) (2012), the original crop" class="md-figure-image"><figcaption class="md-figure-caption">Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (2012), the original crop</figcaption></figure>
<ul>
<li>Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a> (2012), the original crop</li>
</ul>
<p>It does not require an eagle eye to spot it — the top part is accidentally cropped. And so it runs through all subsequent slide decks, references, etc. In my opinion, it is a symptom that, in deep learning research, visualization is a mere afterthought (with a few notable exceptions, including the <a href="https://distill.pub/about/" target="_blank" rel="nofollow noopener noreferrer">Distill journal</a>).</p>
<p>One may argue that developing new algorithms and tuning hyperparameters are Real Science/Engineering™, while the visual presentation is the domain of art and has no value. I couldn’t disagree more!</p>
<p>Sure, for computers running a program it does not matter if your code is without indentations and has obscurely named variables. But for people — it does. Academic papers are not a means of discovery — they are a means of communication.</p>
<p>Take another complex idea — quantum field theory. If you want to show the electron-positron annihilation process, creating a muon-antimuon pair, here’s the<a href="https://en.wikipedia.org/wiki/Feynman_diagram" target="_blank" rel="nofollow noopener noreferrer">Feynman diagram</a> (of the first-order term):</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/738/1*B_7b-pRXOarR4abf3_q4NQ.png" alt="Mark Thomson, [Particle Physics, Handout 4 : Electron-Positron Annihilation](https://www.hep.phy.cam.ac.uk/~thomson/partIIIparticles/handouts/Handout_4_2011.pdf)" class="md-figure-image"><figcaption class="md-figure-caption">Mark Thomson, <a href="https://www.hep.phy.cam.ac.uk/~thomson/partIIIparticles/handouts/Handout_4_2011.pdf">Particle Physics, Handout 4 : Electron-Positron Annihilation</a></figcaption></figure>
<ul>
<li>Mark Thomson, <a href="https://www.hep.phy.cam.ac.uk/~thomson/partIIIparticles/handouts/Handout_4_2011.pdf" target="_blank" rel="nofollow noopener noreferrer">Particle Physics, Handout 4 : Electron-Positron Annihilation</a></li>
</ul>
<p>Cute, isn’t it? But it is not an artistic impression. It is a graphical representation of the scattering amplitude, with each line being a propagator and each vertex — a point interaction.<a href="https://www.southampton.ac.uk/~doug/ft1/ft115.pdf" target="_blank" rel="nofollow noopener noreferrer"> It directly translates to</a>:</p>
<img src="https://cdn-images-1.medium.com/max/1150/1*lxyoi02syfvOihIU1lJ-bQ.png">
<p>I may be biased towards <em>“making things simpler”</em> as I did with complex tensor operations in JavaScript, and visualized their results before it was cool (for<a href="http://quantumgame.io/" target="_blank" rel="nofollow noopener noreferrer"> Quantum Game with Photons</a>). Yet, there is more to the Feynman diagrams analogy than using visual representations for formulae. In both quantum mechanics and deep learning, we do a lot of linear algebra with tensor structures. In fact, one may even use the<a href="https://rockt.github.io/2018/04/30/einsum" target="_blank" rel="nofollow noopener noreferrer"> Einstein summation convention in PyTorch</a>.</p>
<h1 id="explaining-neural-network-layers"><a href="#explaining-neural-network-layers" aria-hidden="true"><span class="icon icon-link"></span></a>Explaining neural network layers</h1>
<p>Before we jump into network architectures, let’s focus on their building blocks — layers. For example, a<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="nofollow noopener noreferrer"> Long Short-Term Memory</a> (LSTM) unit can be described with the following equation:</p>
<img src="https://cdn-images-1.medium.com/max/1000/1*GFZ32jmyLR_QuBxNynqv2A.png">
<p>Sure, it’s reasonably easy to parse these equations. At least — if you are already familiar with matrix multiplication conventions. But it is a very different thing to parse something, and to understand it. When I saw LSTM equations for the first time I could parse it, yet I had no idea what was going on.</p>
<p>By <em>“understanding”</em> I don’t mean some spiritual sense of enlightenment — it may be as pleasing and intoxicating as misleading. Instead, I mean building a mental model we are able to work with (to explain, simplify, modify, predict what-if scenarios, etc). Often a graphical form may be cleaner than a verbal one:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1876/1*2BWMDwn_44nBLhTBTU1pZA.png" alt="Chris Olah, [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (2015)" class="md-figure-image"><figcaption class="md-figure-caption">Chris Olah, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (2015)</figcaption></figure>
<ul>
<li>Chris Olah, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="nofollow noopener noreferrer">Understanding LSTM Networks</a> (2015)</li>
</ul>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="nofollow noopener noreferrer">Understanding LSTM Networks</a> is a wonderful blog post about LSTM cells that explains depicted operations in a step-by-step manner. It gave me a big <em>“Eureka!”</em> moment, turning a seemingly random set of multiplications into a reasonable approach to writing (and reading!) data.</p>
<p>And here is an even more explicit diagram of LSTM below:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1964/1*wGBtlv0xCJu9nBRgX4rcFw.png" alt="Eli Benderski, [Minimal character-based LSTM implementation](https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/) (2018)" class="md-figure-image"><figcaption class="md-figure-caption">Eli Benderski, <a href="https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/">Minimal character-based LSTM implementation</a> (2018)</figcaption></figure>
<ul>
<li>Eli Benderski, <a href="https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/" target="_blank" rel="nofollow noopener noreferrer">Minimal character-based LSTM implementation</a> (2018)</li>
</ul>
<p>In my opinion:</p>
<blockquote>
<p>A good diagram is worth a thousand equations.</p>
</blockquote>
<p>It works for almost any other blocks. We can visualize concepts such as dropout (i.e. switching-off neurons, and rendering their connections irrelevant):</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1228/1*PghKZ1K2Lepg01EGfbtKoQ.jpeg" alt="Srivastava, Hinton et al., [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) (2014)" class="md-figure-image"><figcaption class="md-figure-caption">Srivastava, Hinton et al., <a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> (2014)</figcaption></figure>
<ul>
<li>Srivastava, Hinton et al., <a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="nofollow noopener noreferrer">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> (2014)</li>
</ul>
<p>While <a href="https://www.reddit.com/r/MachineLearning/comments/6j28t9/d_why_do_people_draw_neural_networks_upside_down/" target="_blank" rel="nofollow noopener noreferrer">I am not a big fan of drawing data flows upside-down</a>, this figure is very clear.</p>
<p>Graphical representations are useful for explaining compound blocks, composed of smaller ones (e.g. a few subsequent convolutions). Take a look at this Inception module diagram:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/870/1*VjlEF7Qe6ezY9oD-4h_deA.png" alt="Szegedy, Vanhoucke, Ioffe, Shlens, Wojna, [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567) (2015)" class="md-figure-image"><figcaption class="md-figure-caption">Szegedy, Vanhoucke, Ioffe, Shlens, Wojna, <a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> (2015)</figcaption></figure>
<ul>
<li>Szegedy, Vanhoucke, Ioffe, Shlens, Wojna, <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="nofollow noopener noreferrer">Rethinking the Inception Architecture for Computer Vision</a> (2015)</li>
</ul>
<p>Each visualization is different — not only in the terms of its style but what does it put an emphasis on, and what does it abstract away. What’s important? The number of layers, connections between them, convolution kernel size or activation function? Well, it depends. Abstraction means <em>“the process of considering something independently of its associations or attributes”</em>. The challenge is to decide what is important for a given communication, and what should be hidden.</p>
<p>For example, in this Batch Normalization diagram, the emphasis is on the backward pass:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2400/0*eHSZzD8H9WBuR9Ve.png" alt="Frederik Kratzert, [Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) (2016)" class="md-figure-image"><figcaption class="md-figure-caption">Frederik Kratzert, <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">Understanding the backward pass through Batch Normalization Layer</a> (2016)</figcaption></figure>
<ul>
<li>Frederik Kratzert, <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="nofollow noopener noreferrer">Understanding the backward pass through Batch Normalization Layer</a> (2016)</li>
</ul>
<h1 id="data-viz-vs-data-art"><a href="#data-viz-vs-data-art" aria-hidden="true"><span class="icon icon-link"></span></a>Data viz vs data art</h1>
<p>You may get the impression that I argue for making deep learning papers more visually appealing. Well, it wouldn’t hurt to make charts nicer. When I work with data exploration, I often pick <a href="http://bl.ocks.org/aaizemberg/78bd3dade9593896a59d" target="_blank" rel="nofollow noopener noreferrer">nicer color schemes</a> just to make a more pleasant experience. My main point is to turn visualizations into a more effective means of communication.</p>
<p>So, does nicer mean better? Not necessarily.<a href="https://lisacharlotterost.github.io/2015/12/14/The-Line-between-Data-Vis-And-Data-Art/" target="_blank" rel="nofollow noopener noreferrer"> The Line between Data Vis and Data Art</a> by Lisa Charlotte Rost, which I found very insightful, explains the distinction.</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1788/1*e1mu_o8ATwunBdOmtvj1IQ.png" alt="Lisa Charlotte Rost, [Meaning + Beauty in Data Vis and Data Art](https://lisacharlotterost.github.io/2015/12/19/Meaning-and-Beauty-in-Data-Vis/) (2015)" class="md-figure-image"><figcaption class="md-figure-caption">Lisa Charlotte Rost, <a href="https://lisacharlotterost.github.io/2015/12/19/Meaning-and-Beauty-in-Data-Vis/">Meaning + Beauty in Data Vis and Data Art</a> (2015)</figcaption></figure>
<ul>
<li>Lisa Charlotte Rost, <a href="https://lisacharlotterost.github.io/2015/12/19/Meaning-and-Beauty-in-Data-Vis/" target="_blank" rel="nofollow noopener noreferrer">Meaning + Beauty in Data Vis and Data Art</a> (2015)</li>
</ul>
<p>For example, look this stunning picture below:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1700/1*eWzAOBnDsN3m1aaFMtcM9w.png" alt="Matt Fyles (Graphcore), [Inside an AI ‘brain’ — What does machine learning look like?](https://www.graphcore.ai/posts/what-does-machine-learning-look-like) (2016)" class="md-figure-image"><figcaption class="md-figure-caption">Matt Fyles (Graphcore), <a href="https://www.graphcore.ai/posts/what-does-machine-learning-look-like">Inside an AI ‘brain’ — What does machine learning look like?</a> (2016)</figcaption></figure>
<ul>
<li>Matt Fyles (Graphcore), <a href="https://www.graphcore.ai/posts/what-does-machine-learning-look-like" target="_blank" rel="nofollow noopener noreferrer">Inside an AI ‘brain’ — What does machine learning look like?</a> (2016)</li>
</ul>
<p>Beautiful, isn’t it? To me, it looks alive — like a cell, with its organelle. …but hey — can we deduce anything from it? Would you even guess it’s the same AlexNet?</p>
<p>In another example, an animated multi-layer perceptron is focused on its aesthetic, rather than explanatory, value:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1096/1*izgVEHysbzgvOIV8VR3c9A.gif" alt="Jesús Martínez-Blanco, [Sinapsis](http://chumo.github.io/Sinapsis/) (2016)" class="md-figure-image"><figcaption class="md-figure-caption">Jesús Martínez-Blanco, <a href="http://chumo.github.io/Sinapsis/">Sinapsis</a> (2016)</figcaption></figure>
<ul>
<li>Jesús Martínez-Blanco, <a href="http://chumo.github.io/Sinapsis/" target="_blank" rel="nofollow noopener noreferrer">Sinapsis</a> (2016)</li>
</ul>
<p>To make it clear: data art has value on its own, as long as we don’t confuse artistic value with educational value. If you like going this route, I encourage you to use 3D animations of impulses such as<a href="https://codepen.io/towc/embed/wGjXGY" target="_blank" rel="nofollow noopener noreferrer"> these sparks</a> or that<a href="https://codepen.io/lindell/embed/pePwzM" target="_blank" rel="nofollow noopener noreferrer"> colorful brain</a> — for an actual ConvNet.</p>
<p>Sometimes the trade-off is less clear. This one, is it data viz or data art?</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2012/1*MKtjMPlexFjSaiwJJ8ui3A.png" alt="GoogLeNet from Christian Szegedy et al., [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842) (2014)" class="md-figure-image"><figcaption class="md-figure-caption">GoogLeNet from Christian Szegedy et al., <a href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a> (2014)</figcaption></figure>
<ul>
<li>GoogLeNet from Christian Szegedy et al., <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="nofollow noopener noreferrer">Going Deeper with Convolutions</a> (2014)</li>
</ul>
<p>I guess you said: <em>“data vis, obviously”</em>. In this case — we are in disagreement. While there is a nice color scheme, and the repetition of similar structures is visually pleasing, it is hard to implement this network solely based on this drawing. Sure, you get the gist of the architecture — i.e. the number of layers, and on the structure of blocks, but it’s not enough to reimplement the network (at least, not without a magnifying glass).</p>
<p>To make it clear — there is room for data art in publications. For example, in a network for detecting skin conditions, we see the diagram of Inception v3 feature-extracting layers. Here it is clear that the authors just use it, and represent it graphically, rather than explain its inner workings:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2048/1*Sq8TC0ARvq3rs4z6xwg6rA.png" alt="Andrea Esteva et al., [Dermatologist-level classification of skin cancer with deep neural networks](https://cs.stanford.edu/people/esteva/nature/) (2017)" class="md-figure-image"><figcaption class="md-figure-caption">Andrea Esteva et al., <a href="https://cs.stanford.edu/people/esteva/nature/">Dermatologist-level classification of skin cancer with deep neural networks</a> (2017)</figcaption></figure>
<ul>
<li>Andrea Esteva et al., <a href="https://cs.stanford.edu/people/esteva/nature/" target="_blank" rel="nofollow noopener noreferrer">Dermatologist-level classification of skin cancer with deep neural networks</a> (2017)</li>
</ul>
<p>And how would you classify this diagram, for exploring visual patterns that activate selected channels?</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2032/1*Z7X3ypVDlNGlXeVcJqICbg.png" alt="Chris Olah et al., [Feature Visualization — Appendix](https://distill.pub/2017/feature-visualization/appendix/) (2017), distil.pub" class="md-figure-image"><figcaption class="md-figure-caption">Chris Olah et al., <a href="https://distill.pub/2017/feature-visualization/appendix/">Feature Visualization — Appendix</a> (2017), distil.pub</figcaption></figure>
<ul>
<li>Chris Olah et al., <a href="https://distill.pub/2017/feature-visualization/appendix/" target="_blank" rel="nofollow noopener noreferrer">Feature Visualization — Appendix</a> (2017), distil.pub</li>
</ul>
<p>I would classify the diagram below as a good example of data-viz. A trippy visualization does not make it a piece of data art. In this case, the focus is on network architecture abstraction and presenting relevant data (input images activating a given channel).</p>
<p>Some diagrams abstract a lot of information, giving only a very general idea of what is going on. See the <a href="http://www.asimovinstitute.org/neural-network-zoo/" target="_blank" rel="nofollow noopener noreferrer">Neural Network Zoo</a> and <a href="https://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/" target="_blank" rel="nofollow noopener noreferrer">its prequel</a>:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1222/1*dyuWUs3JfJihpbShcbHKzw.png" alt="Fjodor van Veen, [Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/) (2016), a fragment" class="md-figure-image"><figcaption class="md-figure-caption">Fjodor van Veen, <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a> (2016), a fragment</figcaption></figure>
<ul>
<li>Fjodor van Veen, <a href="http://www.asimovinstitute.org/neural-network-zoo/" target="_blank" rel="nofollow noopener noreferrer">Neural Network Zoo</a> (2016), a fragment</li>
</ul>
<h1 id="explanatory-architecture-diagrams"><a href="#explanatory-architecture-diagrams" aria-hidden="true"><span class="icon icon-link"></span></a>Explanatory architecture diagrams</h1>
<p>We saw a few examples of layer diagrams, and pieces of data art related to neural network architectures.</p>
<p>Let’s go to (data) visualizations of neural network architectures. Here is the architecture of VGG16, a standard network for image classification.</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1364/1*BncGa1UaQiYbYkSjLc39BA.png" alt="[https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/](https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/)" class="md-figure-image"><figcaption class="md-figure-caption"><a href="https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/">https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/</a></figcaption></figure>
<p>We see, step-by-step, tensor sizes and operations (marked as colors). It’s not abstract — box sizes are related to tensor shapes. Bear in mind that the thickness related to the number of channels is not to scale (well, we have 3 to 4096).</p>
<p>A similar approach is to show values for each channel, as in this DeepFace work:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2048/1*KHngh7OcKlkIQeuuUIZk7Q.png" alt="Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf, [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/) (2014)" class="md-figure-image"><figcaption class="md-figure-caption">Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf, <a href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</a> (2014)</figcaption></figure>
<ul>
<li>Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf, <a href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/" target="_blank" rel="nofollow noopener noreferrer">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</a> (2014)</li>
</ul>
<p>Such diagrams are not restricted to computer vision. Let’s see one for turning text into… colors:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1506/1*kI5yEQPcf9X9d4j6JpwTpg.png" alt="Chengwei Zhang, [How to train a Keras model to generate colors](https://heartbeat.fritz.ai/how-to-train-a-keras-model-to-generate-colors-3bc79e54971b) (2018)" class="md-figure-image"><figcaption class="md-figure-caption">Chengwei Zhang, <a href="https://heartbeat.fritz.ai/how-to-train-a-keras-model-to-generate-colors-3bc79e54971b">How to train a Keras model to generate colors</a> (2018)</figcaption></figure>
<ul>
<li>Chengwei Zhang, <a href="https://heartbeat.fritz.ai/how-to-train-a-keras-model-to-generate-colors-3bc79e54971b" target="_blank" rel="nofollow noopener noreferrer">How to train a Keras model to generate colors</a> (2018)</li>
</ul>
<p>Such diagrams might be useful if the goal is to show the network architecture and at the same time — give some hints on its inner workings. They seem to be especially useful for tutorials, e.g. the seminal <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="nofollow noopener noreferrer">The Unreasonable Effectiveness of Recurrent Neural Networks</a>.</p>
<h1 id="abstract-architecture-diagrams"><a href="#abstract-architecture-diagrams" aria-hidden="true"><span class="icon icon-link"></span></a>Abstract architecture diagrams</h1>
<p>However, for larger models, explanatory diagrams may be unnecessarily complex or too specific to show all possible layers within a single diagram style. So, the way to go is to use abstract diagrams. Typically, nodes denote operations, while arrows represent the tensor flow. For example, let’s look at this VGG-19 vs ResNet-34 comparison:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1634/1*zOu3PXhdMBY6KZrDpCXdwQ.png" alt="Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015), cropped" class="md-figure-image"><figcaption class="md-figure-caption">Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> (2015), cropped</figcaption></figure>
<ul>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="nofollow noopener noreferrer">Deep Residual Learning for Image Recognition</a> (2015), cropped</li>
</ul>
<p>We can see that there is some redundancy, as some units get reused or repeated. Since diagrams can be long (there is a reason why I cropped the one above!), it is beneficial to spot the patterns and consolidate them. Such a hierarchy makes it simpler both to understand concepts and present them visually (unless we just want to create data-artsy diagrams of GoogLeNet).</p>
<p>For example, let’s look at this one, of Inception-ResNet-v1:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1844/1*l0h9YFYdyBUvk1E8LBZTQw.png" alt="Inception-ResNet-v1 as depicted in Szegedy et al., [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261) (2016), combined two figures" class="md-figure-image"><figcaption class="md-figure-caption">Inception-ResNet-v1 as depicted in Szegedy et al., <a href="https://arxiv.org/abs/1602.07261">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> (2016), combined two figures</figcaption></figure>
<ul>
<li>Inception-ResNet-v1 as depicted in Szegedy et al., <a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="nofollow noopener noreferrer">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> (2016), combined two figures</li>
</ul>
<p>I adore its composition — we see what’s going on, and which blocks are being repeated.</p>
<p>Another diagram that made a concept super clear to me was one for image segmentation, U-Net:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2048/1*aBLf8WAEbEtJnJef3leQQQ.png" alt="Olaf Ronneberger, Philipp Fischer, Thomas Brox, [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) (2015)" class="md-figure-image"><figcaption class="md-figure-caption">Olaf Ronneberger, Philipp Fischer, Thomas Brox, <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> (2015)</figcaption></figure>
<ul>
<li>Olaf Ronneberger, Philipp Fischer, Thomas Brox, <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank" rel="nofollow noopener noreferrer">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> (2015)</li>
</ul>
<p>Take note that this time nodes denote tensors, whereas arrows represent operations. I find this diagram very clear — we see tensor shapes, convolutions, and pooling operations. Since the original U-Net architecture is not too complex, we can do without looking at its hierarchical structure.</p>
<p>The task of creating clear diagrams get slightly more complicated when we want to use more complex building blocks. If we want to reproduce the network, we need to know its details:</p>
<ul>
<li>Number of channels</li>
<li>Convolutions per MaxPool</li>
<li>Number of MaxPools</li>
<li>Batch normalization or dropout</li>
<li>Activation functions (ReLU? before or after Batch Norm?)</li>
</ul>
<p>As a great example of condensing this level of detail into a diagram, see the diagram below:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1896/1*gB_0B_-zeOoEqq_AOnWccw.png" alt="Arkadiusz Nowaczyński, [Deep learning for satellite imagery via image segmentation](https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/) (2017)" class="md-figure-image"><figcaption class="md-figure-caption">Arkadiusz Nowaczyński, <a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/">Deep learning for satellite imagery via image segmentation</a> (2017)</figcaption></figure>
<ul>
<li>Arkadiusz Nowaczyński, <a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/" target="_blank" rel="nofollow noopener noreferrer">Deep learning for satellite imagery via image segmentation</a> (2017)</li>
</ul>
<p>While the color choice could have been better, I adore its explicit form. There is a clear indication of the number of channels. Each complex layer is explicitly decomposed into its building blocks, maintaining all details (note 3-level hierarchy).</p>
<p>Another interesting approach to the neural network module hierarchy:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1802/1*v7Im24aAolKx_RhxpxYWCA.jpeg" alt="AdaptNet architecture from Abhinav Velda et al., [DeepScene: Semantic Segmentation using Deep Upconvolutional Neural Networks](http://deepscene.cs.uni-freiburg.de/) (2016)" class="md-figure-image"><figcaption class="md-figure-caption">AdaptNet architecture from Abhinav Velda et al., <a href="http://deepscene.cs.uni-freiburg.de/">DeepScene: Semantic Segmentation using Deep Upconvolutional Neural Networks</a> (2016)</figcaption></figure>
<ul>
<li>AdaptNet architecture from Abhinav Velda et al., <a href="http://deepscene.cs.uni-freiburg.de/" target="_blank" rel="nofollow noopener noreferrer">DeepScene: Semantic Segmentation using Deep Upconvolutional Neural Networks</a> (2016)</li>
</ul>
<h1 id="automatic-tools-for-neural-network-architecture-visualization"><a href="#automatic-tools-for-neural-network-architecture-visualization" aria-hidden="true"><span class="icon icon-link"></span></a>Automatic tools for neural network architecture visualization</h1>
<p>You can draw your network manually. Use<a href="https://inkscape.org/en/" target="_blank" rel="nofollow noopener noreferrer"> Inkscape</a> (as Chris Olah did),<a href="http://www.texample.net/tikz/examples/" target="_blank" rel="nofollow noopener noreferrer"> TikZ</a> (if you are a fan of LaTeX) or any other tool. The other one is to generate them automatically.</p>
<p>I hope that you are aware that you already interact with one visual representation — code (yes, a text is a visual representation!). For some projects, the code might suffice, especially if you work with a concise framework (such as<a href="https://deepsense.ai/keras-or-pytorch/" target="_blank" rel="nofollow noopener noreferrer"> Keras or PyTorch</a>). For more convoluted (pun totally intended) architectures, diagrams add a lot of explanatory value.</p>
<h4 id="tensorboard-graph"><a href="#tensorboard-graph" aria-hidden="true"><span class="icon icon-link"></span></a>TensorBoard: Graph</h4>
<p><a href="https://www.tensorflow.org/guide/graph_viz" target="_blank" rel="nofollow noopener noreferrer">TensorBoard</a> is arguably the most popular network visualization tool. A TensorFlow network graph looks like this:</p>
<img src="https://cdn-images-1.medium.com/max/1168/1*7_ycMgp2YgcCLyMvzEDCNQ.png">
<p>Does it provide a readable summary for a neural network?</p>
<p>In my opinion, it does not.</p>
<p>While this diagram shows the structure of computations, some things are long-winded (e.g. adding bias as a separate operation). Additionally, the most important parts are being masked: the core parameters of operations (e.g. convolution kernel size), and tensor sizes. Though, before going into criticising, I really encourage reading the accompanying paper:</p>
<ul>
<li>K. Wongsuphasawat, D. Smilkov et al,<a href="http://idl.cs.washington.edu/files/2018-TensorFlowGraph-VAST.pdf" target="_blank" rel="nofollow noopener noreferrer"> Visualizing dataflow graphs of deep learning models in TensorFlow</a>, 2018</li>
</ul>
<p>This article provides insight into the many challenges of creating network diagrams bottom-up. Since we are allowed to use all TensorFlow operations, including auxiliary ones (such as initialization or logging), it is challenging to make a general, readable graph. If we don’t assume much about what is important to the reader(e.g. that convolution kernel size may vary, but all operations are expected to have a bias), it is hard to make a general tool for turning any TensorFlow computation diagram into a useful (think: publication-ready) diagram.</p>
<h4 id="keras"><a href="#keras" aria-hidden="true"><span class="icon icon-link"></span></a>Keras</h4>
<p>Keras is a high-level deep learning framework and therefore has huge potential for beautiful visualizations. (Side note: if you want to use an interactive train graph for Jupyter Notebook, I wrote one:<a href="https://github.com/stared/livelossplot" target="_blank" rel="nofollow noopener noreferrer"> livelossplot</a>.) Yet, in my opinion, <a href="https://keras.io/visualization" target="_blank" rel="nofollow noopener noreferrer">its default visualizing option</a> (using GraphViz) is not stellar:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1948/1*AUpvvhkmat7KBNIyPA2cYg.png" alt="[https://keras.io/visualization/](https://keras.io/visualization/)" class="md-figure-image"><figcaption class="md-figure-caption"><a href="https://keras.io/visualization/">https://keras.io/visualization/</a></figcaption></figure>
<p>I think it hides important details, while provides redundant data (duplicated tensor sizes). Aesthetically, I don’t love it nearly much as <a href="https://twitter.com/mbostock/status/1030198344423886848" target="_blank" rel="nofollow noopener noreferrer">Mike Bostock does</a>.</p>
<p>I tried to write another one, <a href="https://github.com/stared/keras-sequential-ascii" target="_blank" rel="nofollow noopener noreferrer">keras-sequential-ascii</a> for trainings:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1880/1*jLld9FGjaLHFhukZ0btIyw.png" alt="Piotr Migdał, [Sequential model in Keras -> ASCII](https://github.com/stared/keras-sequential-ascii) (2017)" class="md-figure-image"><figcaption class="md-figure-caption">Piotr Migdał, <a href="https://github.com/stared/keras-sequential-ascii">Sequential model in Keras -> ASCII</a> (2017)</figcaption></figure>
<p>This structure works for small-sized sequential network architectures. I’ve found it useful for training and courses, such as<a href="https://deepsense.ai/deep-learning-hands-on-image-classification/" target="_blank" rel="nofollow noopener noreferrer"> Starting deep learning hands-on: image classification on CIFAR-10</a>. But not for anything more advanced (though, I was advised to use<a href="https://stackoverflow.com/questions/1057564/pretty-git-branch-graphs" target="_blank" rel="nofollow noopener noreferrer"> branching viz like from git log</a>). And, apparently, I am not the only one who tried ASCII art for neural network viz:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1640/1*2O_B3FPZQHWLj1OJcBfP-Q.png" alt="Brian Low, [Keras models as ASCII diagrams](https://github.com/brianlow/keras_diagram) (2016)" class="md-figure-image"><figcaption class="md-figure-caption">Brian Low, <a href="https://github.com/brianlow/keras_diagram">Keras models as ASCII diagrams</a> (2016)</figcaption></figure>
<ul>
<li>Brian Low, <a href="https://github.com/brianlow/keras_diagram" target="_blank" rel="nofollow noopener noreferrer">Keras models as ASCII diagrams</a> (2016)</li>
</ul>
<p>Though, I would say that the most aesthetically pleasing is one found in<a href="https://github.com/transcranial/keras-js" target="_blank" rel="nofollow noopener noreferrer"> Keras.js</a> (an ambitious project bringing neural networks to the browser, with GPU support):</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1316/1*X2wPp_uPof7vMMyga3rwzQ.png" alt="Leon Chen, [SeqeezeNet v.1.1 from Keras.js Demo](https://transcranial.github.io/keras-js/#/squeezenet-v1.1) (2018)" class="md-figure-image"><figcaption class="md-figure-caption">Leon Chen, <a href="https://transcranial.github.io/keras-js/#/squeezenet-v1.1">SeqeezeNet v.1.1 from Keras.js Demo</a> (2018)</figcaption></figure>
<ul>
<li>Leon Chen, <a href="https://transcranial.github.io/keras-js/#/squeezenet-v1.1" target="_blank" rel="nofollow noopener noreferrer">SeqeezeNet v.1.1 from Keras.js Demo</a> (2018)</li>
</ul>
<p>This project is no longer in active development, in favor of <a href="https://js.tensorflow.org/" target="_blank" rel="nofollow noopener noreferrer">TensorFlow.js</a>. Yet, as it is open-source and modular (using <a href="https://vuejs.org/" target="_blank" rel="nofollow noopener noreferrer">Vue.js</a> framework), it may work as a starting ground for creating a standalone-viz. Ideally, one working in Jupyter Notebook or separate browser window, much alike<a href="https://spacy.io/usage/visualizers#section-jupyter" target="_blank" rel="nofollow noopener noreferrer"> displaCy for sentence decomposition</a>.</p>
<h4 id="moniel"><a href="#moniel" aria-hidden="true"><span class="icon icon-link"></span></a>Moniel</h4>
<p>Instead of turning a functional neural network into a graph, we can define an abstract structure. In<a href="https://github.com/mlajtos/moniel" target="_blank" rel="nofollow noopener noreferrer"> Moniel</a> by Milan Lajtoš the best part is that we can define a hierarchical structure:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1638/1*u6uIQF4xTVe-ylJnAPoIDg.png" alt="Milan Lajtoš, [Moniel — Interactive Notation for Computational Graphs](https://github.com/mlajtos/moniel) (2017)" class="md-figure-image"><figcaption class="md-figure-caption">Milan Lajtoš, <a href="https://github.com/mlajtos/moniel">Moniel — Interactive Notation for Computational Graphs</a> (2017)</figcaption></figure>
<ul>
<li>Milan Lajtoš, <a href="https://github.com/mlajtos/moniel" target="_blank" rel="nofollow noopener noreferrer">Moniel — Interactive Notation for Computational Graphs</a> (2017)</li>
</ul>
<p>I like this hierarchical-structure approach. Moniel was an ambitious idea to create a specific language (rather than, say, to use YAML). Sadly, <a href="https://github.com/mlajtos/moniel/issues/13" target="_blank" rel="nofollow noopener noreferrer">the project lies abandoned</a>.</p>
<h4 id="netscope"><a href="#netscope" aria-hidden="true"><span class="icon icon-link"></span></a>Netscope</h4>
<p>I got inspired by<a href="https://dgschwend.github.io/netscope/quickstart.html" target="_blank" rel="nofollow noopener noreferrer"> Netscope CNN Analyzer</a> by<a href="https://github.com/dgschwend" target="_blank" rel="nofollow noopener noreferrer"> dgschwend</a> (based on a project by<a href="https://github.com/ethereon" target="_blank" rel="nofollow noopener noreferrer"> ethereon</a>). It is a project with many forks, so by now a different one may be more up-to-date:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/1546/1*yJl3F1gIPE2IK5y3yYa-lQ.png" alt="David Gschwend, Saumitro Dasgupta, [SqueezeNet v.1. from Netscope CNN Analyzer ](https://dgschwend.github.io/netscope/#/preset/squeezenet_v11)(2018)" class="md-figure-image"><figcaption class="md-figure-caption">David Gschwend, Saumitro Dasgupta, <a href="https://dgschwend.github.io/netscope/#/preset/squeezenet_v11">SqueezeNet v.1. from Netscope CNN Analyzer </a>(2018)</figcaption></figure>
<ul>
<li>David Gschwend, Saumitro Dasgupta, <a href="https://dgschwend.github.io/netscope/#/preset/squeezenet_v11" target="_blank" rel="nofollow noopener noreferrer">SqueezeNet v.1. from Netscope CNN Analyzer </a> (2018)</li>
</ul>
<p>It is based on Caffe’s <code>.prototxt</code>format. I love its color theme, the display of channel sizes and mouseover tooltip for exact parameters. The main problem, though, is the lack of a hierarchical structure. Networks get (too) big very soon.</p>
<h4 id="netron"><a href="#netron" aria-hidden="true"><span class="icon icon-link"></span></a>Netron</h4>
<p>Another ambitious project: <a href="https://github.com/lutzroeder/netron" target="_blank" rel="nofollow noopener noreferrer">Netron</a> by Lutz Roeder:</p>
<figure class="md-figure-block"><img src="https://cdn-images-1.medium.com/max/2050/1*6nDySAw15yh2GHyevFYvnQ.png" alt="Lutz Roeder, [Netrone — Visualizer for deep learning and machine learning models](https://github.com/lutzroeder/netron) (2018)" class="md-figure-image"><figcaption class="md-figure-caption">Lutz Roeder, <a href="https://github.com/lutzroeder/netron">Netrone — Visualizer for deep learning and machine learning models</a> (2018)</figcaption></figure>
<ul>
<li>Lutz Roeder, <a href="https://github.com/lutzroeder/netron" target="_blank" rel="nofollow noopener noreferrer">Netrone — Visualizer for deep learning and machine learning models</a> (2018)</li>
</ul>
<p>It is a web app, with standalone versions. Ambitiously, it reads various formats</p>
<blockquote>
<p>N<em>etron supports <a href="http://onnx.ai/" target="_blank" rel="nofollow noopener noreferrer">ONNX</a> (<code>.onnx</code>, <code>.pb</code>), Keras (<code>.h5</code>, <code>.keras</code>), CoreML (<code>.mlmodel</code>) and TensorFlow Lite (<code>.tflite</code>). Netron has experimental support for Caffe (<code>.caffemodel</code>), Caffe2 (<code>predict_net.pb</code>), MXNet (<code>.model</code>, <code>-symbol.json</code>), TensorFlow.js (<code>model.json</code>, <code>.pb</code>) and TensorFlow (<code>.pb</code>, <code>.meta</code>).</em></p>
</blockquote>
<p>It sounds awesome! Though, it is a bit more verbose than NetScope (with activation functions) and, most fundamentally, it lacks the hierarchical structure. But for a general visualization, it may be the best starting point.</p>
<h4 id="edit-other-tools"><a href="#edit-other-tools" aria-hidden="true"><span class="icon icon-link"></span></a>EDIT: Other tools</h4>
<p>A few other tools that may be useful or inspiring:</p>
<ul>
<li><a href="http://alexlenail.me/NN-SVG/LeNet.html" target="_blank" rel="nofollow noopener noreferrer">NN-SVG: LeNet- and AlexNet-style diagrams</a></li>
<li><a href="http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/" target="_blank" rel="nofollow noopener noreferrer">Visualizing CNN architectures side by side with MXNet</a></li>
<li><a href="https://tensorspace.org/" target="_blank" rel="nofollow noopener noreferrer">TensorSpace.js</a> — an in-browser 3D visualizations of channels (stunning but hardly useful)</li>
<li><a href="https://github.com/waleedka/hiddenlayer/" target="_blank" rel="nofollow noopener noreferrer">HiddenLayer</a> — diagrams with ONNX &#x26; Graphviz in Jupyter Notebook for TensorFlow, Keras and PyTorch</li>
<li><a href="https://github.com/HarisIqbal88/PlotNeuralNet" target="_blank" rel="nofollow noopener noreferrer">PlotNeuralNet</a> — LaTeX code for drawing convolutional neural networks</li>
</ul>
<p>And a few threads:</p>
<ul>
<li><a href="https://www.quora.com/What-tools-are-good-for-drawing-neural-network-architecture-diagrams" target="_blank" rel="nofollow noopener noreferrer">What tools are good for drawing neural network architecture diagrams? — Quora</a></li>
<li><a href="https://datascience.stackexchange.com/questions/12851/how-do-you-visualize-neural-network-architectures" target="_blank" rel="nofollow noopener noreferrer">How do you visualize neural network architectures? — Data Science Stack Exchange</a></li>
</ul>
<h1 id="conclusion-and-call-for-action"><a href="#conclusion-and-call-for-action" aria-hidden="true"><span class="icon icon-link"></span></a>Conclusion and call for action</h1>
<p>We saw quite a few examples of neural network visualization, shedding light on the following trade-offs:</p>
<ul>
<li><strong>data viz </strong>vs<strong> data art</strong> (useful vs beautiful)</li>
<li><strong>explicit </strong>vs<strong> implicit</strong> (should I show ReLU all the time? But what about tensor dimensions?)</li>
<li><strong>shallow vs hierarchical</strong></li>
<li><strong>static</strong> (works well in publications) vs <strong>interactive</strong> (provides more information)</li>
<li><strong>specific</strong> vs <strong>general</strong> (does it work for a reasonably broad family of neural networks?)</li>
<li><strong>data flow direction</strong> (top to bottom, bottom to top, or left to right; hint: <a href="https://www.reddit.com/r/MachineLearning/comments/6j28t9/d_why_do_people_draw_neural_networks_upside_down/" target="_blank" rel="nofollow noopener noreferrer">please don’t draw bottom-to-top</a>)</li>
</ul>
<p>Each of those topics is probably worth a Master's thesis, and all combined — a PhD (especially with a meticulous study of how people do visualize and what are the abstractions.)</p>
<p>I think there is a big opportunity in creating a standard neural network visualization tool, as common for neural network architectures as <code>matplotlib</code> is for charts. It remains a challenge at the intersection of deep learning and data visualization. The tool should be useful and general enough, to become a standard for:</p>
<ul>
<li>tutorials in neural networks</li>
<li>academic publications</li>
<li>showing network architecture to collaborators</li>
</ul>
<p>If we want to make it interactive, JavaScript is a must. Be it D3.js, Vue.js, React or any other tech. That way, it is not only easy to make it interactive, but also system agnostic. Take <a href="https://bokeh.pydata.org/" target="_blank" rel="nofollow noopener noreferrer">Bokeh</a> as an example — being useful within a Jupyter Notebook, but also — as a standalone website.</p>
<p>Would you like to start a brand new package? Or contribute to an existing one?</p>
<p>If you find any neural network particularly inspiring, or confusing, share it in the comments! :)</p>
<h1 id="afterwords"><a href="#afterwords" aria-hidden="true"><span class="icon icon-link"></span></a>Afterwords</h1>
<p>This article is based on my talk “Simple diagrams of convoluted neural networks” (<a href="https://pydata.org/berlin2018/schedule/presentation/30/" target="_blank" rel="nofollow noopener noreferrer">abstract</a>,<a href="https://www.dropbox.com/s/a7xako61ihuh82k/20180707_network_viz_pydata_berlin.pdf?dl=0" target="_blank" rel="nofollow noopener noreferrer"> slides</a>) from PyData Berlin 2018 (BTW: and I invite you to<a href="https://pydata.org/warsaw2018/" target="_blank" rel="nofollow noopener noreferrer"> PyData Warsaw, 19–20 Nov 2018</a>). Typically I write on my blog<a href="https://p.migdal.pl/" target="_blank" rel="nofollow noopener noreferrer"> p.migdal.pl</a>. Now I give Medium a try, as it is easier to include images than with<a href="https://p.migdal.pl/2015/12/02/first-post.html" target="_blank" rel="nofollow noopener noreferrer"> Jekyll</a>.</p>
<p>I am grateful to <a href="https://medium.com/u/1631d8afa706" target="_blank" rel="nofollow noopener noreferrer">Ilja Sperling</a> for fruitful conversations after the talk and to <a href="https://medium.com/u/72ff3f0101ef" target="_blank" rel="nofollow noopener noreferrer">Rafał Jakubanis</a> and <a href="https://medium.com/u/39462454f46a" target="_blank" rel="nofollow noopener noreferrer">Sarah Martin, CSC</a> for numerous remarks on the draft.</p>
</div></div>
    <script src="/assets/js/app.73194795.js" defer></script><script src="/assets/js/page--src--templates--blog-post-vue.1cb9f861.js" defer></script>
  </body>
</html>
