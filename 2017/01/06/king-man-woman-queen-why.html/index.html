<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>king - man + woman is queen; but why? - Piotr Migdał</title><meta name="gridsome:hash" content="b0077dd5c1903d3c706ab8cc7617e9f7f006295a"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" property="og:title" content="king - man + woman is queen; but why?"><meta data-vue-tag="ssr" name="twitter:title" content="king - man + woman is queen; but why?"><meta data-vue-tag="ssr" name="description" content="Words, vectors, analogies and conceptual metaphors - the linear space of word2vec and GloVe. Or: how to change gender with a vector."><meta data-vue-tag="ssr" property="og:description" content="Words, vectors, analogies and conceptual metaphors - the linear space of word2vec and GloVe. Or: how to change gender with a vector."><meta data-vue-tag="ssr" name="twitter:description" content="Words, vectors, analogies and conceptual metaphors - the linear space of word2vec and GloVe. Or: how to change gender with a vector."><meta data-vue-tag="ssr" name="url" content="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/"><meta data-vue-tag="ssr" property="og:url" content="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/"><meta data-vue-tag="ssr" name="twitter:url" content="https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/"><meta data-vue-tag="ssr" property="og:image" content="https://p.migdal.pl/assets/static/queen-julia-vectors-facebook.6ef341c.e28cc5a16e23876b41bd2278ea34bac3.jpg"><meta data-vue-tag="ssr" name="twitter:image" content="https://p.migdal.pl/assets/static/queen-julia-vectors-facebook.6ef341c.e28cc5a16e23876b41bd2278ea34bac3.jpg"><meta data-vue-tag="ssr" name="author" content="Piotr Migdał"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/assets/static/favicon.62d22cb.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/assets/static/favicon.1539b60.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/assets/static/favicon.dc0cdc5.9bb7ffafafc09ac851d81afb65b8ef59.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/assets/static/favicon.7b22250.9bb7ffafafc09ac851d81afb65b8ef59.png"><link rel="preload" href="/assets/css/0.styles.a9fa044d.css" as="style"><link rel="preload" href="/assets/js/app.5b89ac61.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--blog-post-vue.bf85aef2.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.c98891ed.js"><link rel="prefetch" href="/assets/js/page--src--pages--blog-vue.e77a3335.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.becc1ed5.js"><link rel="prefetch" href="/assets/js/page--src--pages--projects-vue.789bf31f.js"><link rel="prefetch" href="/assets/js/page--src--pages--publications-vue.42f26c55.js"><link rel="prefetch" href="/assets/js/page--src--pages--resume-vue.ffce1cbc.js"><link rel="stylesheet" href="/assets/css/0.styles.a9fa044d.css"><script data-vue-tag="ssr" src="https://plausible.io/js/plausible.outbound-links.js" async defer data-domain="p.migdal.pl"></script><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="layout"><header class="header"><strong><a href="/" class="active">Piotr Migdał</a></strong><nav class="nav"><a href="/blog" class="nav__link">Blog</a><a href="/projects" class="nav__link">Projects</a><a href="/publications" class="nav__link">Publications</a><a href="/resume" class="nav__link">Resume</a></nav></header><div class="markdown-header"><h1>king - man + woman is queen; but why?</h1><p class="header-information">
      6 January 2017 | by Piotr Migdał
      <!---->
      | 11 min read
    <ul class="header-extras"><li><a href="https://news.ycombinator.com/item?id=13346104">Hacker News discussion thread with over 250 upvotes</a></li></ul></p></div><div class="markdown"><h2 id="intro"><a href="#intro" aria-hidden="true"><span class="icon icon-link"></span></a>Intro</h2>
<p><strong>word2vec</strong> is an algorithm that transforms words into vectors, so that words with similar meaning end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example the famous <code>king - man + woman = queen</code>.</p>
<p>I will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible.</p>
<p>If you would rather explore than read, here is an interactive exploration by my mentee Julia Bazińska, now a freshman of computer science at the University of Warsaw:</p>
<ul>
<li><a href="https://lamyiowce.github.io/word2viz/" target="_blank" rel="nofollow noopener noreferrer">Word2viz</a> by using <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="nofollow noopener noreferrer">GloVe</a> pre-trained vectors (it takes 30MB to load - please be patient)</li>
</ul>
<p><a href="https://lamyiowce.github.io/word2viz/" target="_blank" rel="nofollow noopener noreferrer"><div class="images">
<img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 681 494' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-18f3a4e0d7ab4dda69267a0d3aad52f3'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-18f3a4e0d7ab4dda69267a0d3aad52f3)' width='681' height='494' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAuCAYAAACYlx/0AAAACXBIWXMAAAsSAAALEgHS3X78AAAISElEQVRo3uVaaZPjRBL1//8ZCwuzEctyL8QsQzPwBQgYeht6%2bqC73T5kWz50WLJ119vM0uGSLMl2j3u2AUVklFW6qrIyX77McieOY3S7XYRhBD6EEFLy33/2oxOFIT779EOYtlOa9F9h8lIBIknw4QfPMF2YsiOKYqxWq/%2b7EtSFqMoxx9ahV%2bHZe%2b/grq8VncvlstYdSoNQ2mOL%2bv5HV0ASR/j25QlmZAG%2bt8JiYcFxNu6QfW57hR7bAvawjiNZwOaYjIa4uu7CNM3NhzIl%2bL5PillivV5BJDH1J2D34VZKokhxTveRgqVEIUkAhLtFyNZHQsA8mxuYTMaYjEcYjXUwaB/VAlKzSkovVC2A1yKhZj4zMOp1ofXuEbhLORkReHKgrcKT5snniuAJ7BARp4pjBburtVwQl8bkEjY9AgZgy69LLiDKBrlxi8d1g7fqAvkLx6MBLi6vyy4AkWGTUD78eAD41kFQfZnnrWUE2CsKtPT9oXhAnVmVMaDG/ETZKmon/wdRRMkCVq4Lw7RaFbDxT1Hvl3VmrfY/ZQVIDLi4qmDAtgXk/eu1j/vbW2iDPjHIKMWGetR6skp5kAvkkYBp85qsxiN%2bkIfEPIbLkCc5gmiC8malQDxdBbT5uOxh8sPC8b/KE1gpkkQ9RClvwQUmY43MeVzrAs3onyjXyhxB5PdmShFRsE2Udihl35D40OhTBsGVS7IuhcFDSEgpSWojNnw9p8qhopTcYnLWKMSjk6K9XUDVcuAHMGYzzKc6pvoE8/kcrltOoRNqGRhDkkSUraLZ/LMcQtLnVDE5tsSUFximjdlsCmOxoHZOC%2bWgymRLhKqKWzUhu2QBnOjY9nKnAnwvgDmfQR%2bNsNB1zCmJcsxFtpIevJUHvXeH0e01%2br9fwl3MaEVDCZCC8gIhEyNqc/PfRYlZmUEI2%2bFsdQHTsjAlBdjLMmUXFdfDHkyypIDRsI/zi%2bvWMIhdwMjCiQyvYJ4F0uQRrEnISnyXNO1AkJCmSPicXG5ly374643w6nPSJRXFLhFnkSVrlYnKaeeTJitiC0xEfVhW59WpY3F1GFBlgCodqgIfD80LAjhkCY7nw6KVMh0iWWxdZD1x1aqKlDrepM25C2TKEIVQRsj9WTbKkvgeIrJebqXLqNOmzNIwl5TNTonnjOGTJTVawK4wqJpQ/66P/3z%2bKb78%2bJ94/2/v4uXzL%2bDZFtyZjrvfzvDLdy8x%2bu0njC/PMLk8hXb1K/TfzzHv3aar7bEVLNNVl20ubAWr1AKq4MiTLsIrW0fqcivXw7DXg9a/x2Cg4fb2Bq9pDGdnv8Imd/EJs2weG%2bFUnCRlBWxSXnaBAUbapNEFhBKbPVpJxzLh2CZs04BtLFK/pgHGNMDYzwbIEtPAXVr9yQBG/xb2sIvE0EnTBgQJXCtViioOjWFJ15aLTObyXiHvXZKiJ5j1u1jqI8wHd5h0b%2bDoGj3q4PsffsTJNy/w/MW3cuI7XSDv4IoPa6k1DO7MEeoP14swHY8xJeC0CEDZXHmlRYEDTtryynNfTbFFugPdl1C4NijyjImCjwcDTLQBdG0IlxYjIXZq2y4W9A3btqENh5gbRmPJv1PK7w9wgSZ0jWJBFmHSAOawLFuW0nYXPTLLyhmkWj6TIBoX1Fq05g8EfHSv53kU0dby29wGYVgZ81ZJDEU22BQG9y1jh5HAYjqF1uti3O/Rak9T35XAFW4kY3%2bilRYnilLSd7BlpOE0St9TWQAcSJpKLjAgEHm9IxvEA1iZ7I/jDMQyHpCDWy4S5MKNNPCDlBckGd6EBTjGhO66PiM6P8FwqGHIrjEeoXvfo/OhXNg62lzrAm0Y8OCNjDqMUM25CIEZbwiDRgUVFsQuRwyRTZ7dbURhjtsVmT2bPvfnv8MoalaAqBCGh7hAmxJKEaSa6e2b7fGqZzmCJEa06gn5%2bGJhyxg/oFUfjzRJlx/gAunJmJD05uaOtGgeJdHYoa364kiTHFg1bs8cGzDApjCSJhn20yhwNihoV4p8SIH2oDD4lI5jbZ521M0Pj8DCIc5uWdYbKWCfUvrbqvjsqYCM31Psfn15W9DHN7GAvYoph2yYPJKiOqghEm/qAkXdIIgx13WiqkMsFaWKQ6vBFZ8/5q5Rp67zWAoIwpj4uQ2DwtTasYsKjyhie7BJfXMKnCStEUDguCUy6QIuUWBucyb4phjQlkiVDnX7PJc6hqhsm4usZuhRisvj5JDNLTO9pJLq7qWAiF768Uf/gmEtEVCGZpqWzKLa4unRBNgtfJ/cMo82QqzOprR3Ok3rgzqX5aiN4%2bSgcUsFxGR2n3z0AaWMm7yZLeKvcLASOj7x5ZOTr7Hy/KyClMCg/DmkFDIIAtnmIqu8yrkq/M%2bNal/%2bfN1z%2bbXm5yLluaD2nfy7Ktwvr4XB1jvza3xeWAC7wD%2bevYe5mSZAXDLiyitbARdH%2bB9jubDJMUCqfSx8L5th9RonIRxSZ7OZ/L19zZbvrL4vf246nTVcs2UpvvYa4QFfq/sez4uFn4%2by5KjDJcwXXz2XJec2APH9NS7PL%2bSeQPVYr5Z4dXomgal6WMYcN9f3te%2b0DBOnr/5b1OnUgyPH6ek5Xdsez3gywevzq9qxjrQ%2bLq4ua7/HVe%2brq%2btSTtApJ2V5HE3kivLGg9iqBGHnFtVxruGoQJwWfBxomlbaxOmg4eaXX3%2bFf3/xZVH8TlG7PqsSKlkp0uvKeYmkiFJbt8enKjz/mw4qmVzpOSj31WSFfLz65Wc8%2b/u7eHHyjaIAiIYPJwQcfjG5tg3Jfa6VdnAOeK79Glq/V2W5OegyICoK2N71qf7VRfxJQl7d%2bf8AZFpj9E338E8AAAAASUVORK5CYII=' /%3e%3c/svg%3e" width="681" data-srcset="/assets/static/word2viz-queen.2008fcb.a8850d043b12c5b88918b4b33ccb257f.png 681w" data-sizes="(max-width: 681px) 100vw, 681px" data-src="/assets/static/word2viz-queen.2008fcb.a8850d043b12c5b88918b4b33ccb257f.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/assets/static/word2viz-queen.2008fcb.a8850d043b12c5b88918b4b33ccb257f.png" width="681"></noscript></a></p>
</div><h2 id="counts-coincidences-and-meaning"><a href="#counts-coincidences-and-meaning" aria-hidden="true"><span class="icon icon-link"></span></a>Counts, coincidences and meaning</h2>
<blockquote>
<p>I love letter co-occurrence in the word <em>co-occurrence</em>.</p>
</blockquote>
<p>Sometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.
Consider this sentence:</p>
<blockquote>
<p>A small, fluffy roosety climbed a tree.</p>
</blockquote>
<p>What's a <em>roosety</em>? I would say that something like a squirrel, since the two words can be easily interchanged. Such reasoning is called the <a href="https://en.wikipedia.org/wiki/Distributional_semantics" target="_blank" rel="nofollow noopener noreferrer">distributional hypothesis</a> and can be summarized as:</p>
<blockquote>
<p>a word is characterized by the company it keeps - <a href="https://en.wikipedia.org/wiki/John_Rupert_Firth" target="_blank" rel="nofollow noopener noreferrer">John Rupert Firth</a></p>
</blockquote>
<p>If we want to teach it to a computer, the simplest, approximated approach is making it look only at word pairs.
Let <em>P(a|b)</em> be the conditional probability that given a word <em>b</em> there is a word <em>a</em> within a short distance (let's say - being spaced by no more that 2 words).
Then we claim that two words <em>a</em> and <em>b</em> are similar if</p>
<p>$$ P(w|a) = P(w|b) $$</p>
<p>for every word <em>w</em>.
In other words, if we have this equality, no matter if there is word <em>a</em> or <em>b</em>, all other words occur with the same frequency.</p>
<p>Even simple word counts, compared by source, can give interesting results, e.g. that in lyrics of metal songs words (<em>cries</em>, <em>eternity</em> or <em>ashes</em> are popular, while words <em>particularly</em> or <em>approximately</em> are not, well, particularly common), see <a href="http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/" target="_blank" rel="nofollow noopener noreferrer">Heavy Metal and Natural Language Processing</a>.
See also <a href="http://juliasilge.com/blog/Gender-Pronouns/" target="_blank" rel="nofollow noopener noreferrer">Gender Roles with Text Mining and N-grams</a> by Julia Silge.</p>
<p>Looking at co-occurrences can provide much more information. For example, one of my projects, <a href="http://p.migdal.pl/tagoverflow/" target="_blank" rel="nofollow noopener noreferrer">TagOverflow</a>, gives insight into structure of programming, based only on the usage of <a href="http://stackoverflow.com/tags" target="_blank" rel="nofollow noopener noreferrer">tags on Stack Overflow</a>.
It also shows that I am in love with pointwise mutual information, which brings us to the next point.</p>
<p><a href="http://p.migdal.pl/tagoverflow/?site=english&#x26;size=32" target="_blank" rel="nofollow noopener noreferrer"><div class="images">
<img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 669 574' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-b960963dd6a0accc6d1423465fbff5ad'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-b960963dd6a0accc6d1423465fbff5ad)' width='669' height='574' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA3CAYAAAC8TkynAAAACXBIWXMAAAsSAAALEgHS3X78AAASE0lEQVRo3s1bC5Bb1XlWZ0JImYY2oSSQtJOSx0yYSRzatBOaoUPbSTswpaWEyRAaSkgyZGAKE%2boSMDWxSTDGhEIIMd6E8rANsQ3G2I4dP9brXXvX3l3tSlppJe3quXqstNLqtXq/pa//f3Tv3aurfeI16fXc0erq3HvP%2bc//f//3/edYhxUczWZT%2bZT/Xq6t%2bmg0Gyt4Nn4nh265wcgdbDTmB2Ho8yDsS9DA6iiVy6hUKqjVagsaSr6v39ODHYPPIpfNoVgsirNcKXe0b77PltAtNWj%2bbNT5nB%2b8zWrHk3cfxIk9IzCPGzFmMsNsGkf/ufPI0OC0HlNv1MXf23oex9deXwf98AhM5jGcP38ek5OOBT2Mv/J9tUZN8Z6VeN%2baeoD2ZalUCj09PdDrhxGZjqOQL4nONWmGG40mqpIHqE/5OaVSSZzRWATVak3xDPnUDo6%2btYeQ1EZ7j/be92KgJT0gOZvBSI8bZqMVfX29iMcTSz5Mni1hGOk57OrValVpMzc31/E%2b9cAaksecdZ3C5pPrYQ%2bPYzU4tSYG4Bnl4%2bhOIx676Qhee%2b4QvD4HPA4HbBYLxoxmjI0a4HS4UKk2xJzJR7FYUv4uFAoCG9RYkM/nhVEW6rTcZjoRwF//76dxzY5L8I29f4dQYBrhmRlMBwMIh8PwTPkRCs3A73YiPB1Cvd5YYw%2bQDOBzRHFghx5BbxRz6RTmkkmkkgnMzsboM4lMJotavdWWZ3nb6cdxy6%2b/gvVHv4dwPLTITDWEF/Bg61KMy94iGyAUD%2bLvX/sCPt11Kb79zj9jnHDG4XLC43LA6/FgnHDI6XTDYRuH2%2b2m8KuvnQFW61KcCfjYY3gNn3npQ/jy61fjmpcuwY%2b7H0aTQH4uPYdsLot0OiNmX/aCSrHSNvP1el1kk3K55UHmoAEvnH4KE37b%2bx8CCqg0pCygAhvtWZcMsO30Rlz7q8tx4xufx7rX/hj37L0FphEDRilcrPZxGAwG2G1O5T12vxXH7AdRKBU6ByOFVINsUavUyNPSIiybbVhxkUBwsXSoRXf5erVKM0mea/YbcMMrn8O1v/xDrOv6GI6Mv7PoSzkj3LHva/jkz3V4qnsDEjMpeDxuRCJRyfANCq0a5jIp5It58oqy4j1rzRl0K3Gr%2bRfNxyrHPA%2bE3Zbdlw93xImd%2bi4YfEPKTKpnSY5xvu/Ot/4Rf7b9A3iGcCM0FYZj0g6v1ys4gJhhwoZamQBUSiDZbFYBz9bvzTUxhm6lg69X64vGW2tw9WU9SJ0qp5MBHDEfQL6Qb7uPjSkzxKmoBwdG96BSa%2bEFe8Jaxv%2bKPaBQLOCfdn0FL/Y8BZ8zCKvNCpfLTacLUcoIihFocFXqrMz%2bFjNCU6UN2JO0XsLXqqUq7t5/C66mMHmu%2bwmkInOYdDowabfC6XHC5w/CH/Ahly%2boiJWEHqtgjSvCAO7QT89sQrftN5gNxxAMBCgnhxAMTiMlERs1918MO7QaoVqrEo%2boKFxBvs7xzyN59Oh9uO7lq7B7oIu0R5iosx1WyxjsExNwOX0I0PsrqntlVdW8GCGw0hS0EgMoGYTcndurXVtcY0%2bqVBGNRuGPTXW8L5Kcwb/uvQGHLfsw7QlhOjQND/EBj9e/at2gm7%2bhIVmuE1hapKVOnaspRKm5CAhpDbCUwWTwZCDlOKcAECHEHhGPxxURpqbLfMwmo3jwwLfRYz0Gz%2bQUpqZ8mCKC5PH4JFm9CgOo47HZXDjfLyQ%2btINcKgQWS6XybIvvqv5mmDRJ4Mjg2qYu6Z46Mb/0bHYxf1xVbUF4QJnISCGdWDWCajmBWuVxLof0uRgO8GDE39T0taGX8N13b8OzPZsRi8faRFXruU0FK9hjorNR%2bl4VXtKuElfpAaVsCtmur6Lw9MeRcQ0gTxknl23R1lwuJ3I2C5wSZYIs0do0gR7/zi/XzmyHB6g8q9FhBALXeusZb5l24jPbP4QvvXIl0egP4qd9m9oZYbPRVpDhfmUyGSVjXAhB0mUCFpQfJUd4SAfHWxvRb3Bg4OwZOOyTGB4cwcSEi4RHgIiKDZYJN4b1RgxbJlFRuVs7fa6Lbgf2vgHz7Tdj%2bsihRV9erpSEXlh/5Hv44isfxY27P4%2b/IC3x9TdvJKHjgNFiht1ux9iYFYZRE/RGg0i76XRaIUUyERNGataXBV7tqWO7ps/9EqmjG1DKZ%2biBVeTJwuwBfNZkCzMbO/s8Cjv%2bCrnd/4JcwNiaHQJGBSMkUJsj6Tp23Wdh/5MPY%2byr60g%2bG2EwmUgXWOGmXH7u/HmcOXMWQZK2wgOMu/DZ7b%2bPda9eITzg2d5NqBcbSJHHidJZqUyznkcmy%2bqzLgCyXK7MA3StsSwGtHCm2ZYu29JgU9OYUxO7WjpFErhAL%2b1%2bGrWHqfmP6CSPyW79GErp2QVfVKBO277zTYx96iOwP3gvEhQ2iUQC6WwOWXLdWCxGyD0l2rVGAewc7sL3D34DPzuzRdQRZABc6OBncVaSWWU8E8OT3Y8IcTVPwRfnA%2boym66V1mgWKR61WUA2TanaxNwv/hLYpEPjJ5cAWy5F4791CHS/iAlPGPZxC6a8U0ROPIQZrZyep0GHBs6iLA9ykUMBwmrLENxrnnE5RcpSWZAjSUpz3m/ZrTWQU5PHcPlzOtz65vWwGiwwGEdgNpsxTl43qNdT2I7ASn30OScxYTVDPzSMwcFh4V26xSq5yneJH6R33QpsoNnf8kHgJzpUyQDBof1weIOwjJkQJGpqt00gncmJUdQbjXatIFJYe02PB8VAKIC2VBQzwzqAvU8xjAoMk5kE7nr7Zty2528QmPURMJdFyvRPe/HMyc04MX4YxWxReFCFn0PPLUgVaD4r5ZK4Jq5TG8UDtGiuRnI2AM/F3NQocts%2bgSoZofKYDqn934faQTlF8SnChkCKK0Y5cveKdjBSzlc7aKVcFWgul9aFFqjOX2OjcNnBFXbguq6r8IWXr8Bvzx3AwJlB9Pb20eyOwWXzwz3pXb0YWoqo8GeFO8IdYBBMzCBj3IuMow81CTfUREW%2bhzvNFi5JlmYpy2mLjcMzUeKqD92inzqHp09tJNXnbbtfKELJcNxeGIb%2bsTzutZ1Ev7uHQqsinq/ENRpt5bUVZwFlVggHILCgrsSmWuvXmXR0MK5mR2lbHoC6EqxWeezqrC4T8QRu2vVlXEFqb/2Be2AzmHGa0q/VaoNt3IYx05gofqrJVWouBXa7ZqWV/qBSfmqUXxUPkBvzjMpdbtXm5l1XdkvB6ihmG0yCtAsojXlkrUqrRUsKE2q%2bqfshXP/qNXh9YDuCUwEECdySyRSSiSQikQjCdMqozYVU9iCuMbKCXExszV9fBRUu5VJI772LgO425OLBzr6yRJVoqHawCxmhJgH6Qt6hvqdGnD6cmK8eM8JzuMgDFG3K5InFqkh9/Js6O6zFipEwwNypraivb%2bX30J77YJ0KweV2Efsy0KdbkI7FQFI7qBK5qffgOygk4ovWFzp1RHuBhGe5kClgwNErEH9b9%2bOiiix75UpV54oNkLEdQ/FxyvGbdXAefQEGqwPT00E6p4Umr8hskImJhBMdBpDrgi%2b/hBGdDpZtT8BOzxgdGsQYxXVDm26b81WkDj0hGWTLyQ24/Hkd/mH3lzA%2bpqf8bYBeb8bMTHTNvEAnV1DyARNmLD2o0DNLhLwcg0lKZWqkVbNGbRmrLpCacrVtHNYf3IfI6DCSFLNBvx%2bR6OwKYrYzlIIJPx45eh8OW94WGSBfyIlQFHgkwFZaPG2%2bdyPomlKuF8taVY7DnPIj53O/n5ehppErUY53D2Cu73mUssklH1pcQjo3xKy35O2iJbOmajleqnix%2b/NkyFUkORS0i6cLheiKKkKC8BC6M9q2bpJmgq6XKjX47cPI//gPgP/SIf7GnQjH0ggGgmScEMLhaYRI2IRCYeE9ZebyGsxQV3SA5TdZKHyimG8xRmnW%2bRrziEatCdfMJNxR5wWtFum0jVnv10imUqC3Fj2kzqb8VuQ3Xwr8UIfwy7dQXE/ATLLY5fIQBTbDNjlJstmFXD6vKLW24oe03jjkDOPWF3sx4gpzrR15MhhXf1gzFFh9qrKNYIE082q3l5/HZbHrX74Gf7vzWtgcVkwHg/D6vJgO%2bOF0e9rCboU1wVbDcrWOVLEpaC5fYiaWJETPUnxk7CeQOv4jFFIR8Jooc2t1oUJ%2blnb1t7VRovX3K6et0N27H11HhuGdGMcAiRIWLDazESP6UQXghACigTOfkCmxGvh4vfG7%2b7%2bOB458C26PG4FggEQSeWTAB7cviOjsqg3QenAhGUJ85%2b1IdT%2bFORI2hXy%2bHfxk0Sa5NA%2bWJS5TXm2Vt7lAGHBGsfsjbS4tiyVtwZRXfYukDIuldlLGIcBVY6UzaxICjZaFk91bgQd1KG/Uwdr3rrQFxgSb3SWKH2hUW7U4jdqT6wcZAs42o6nK3225X5VFRKGzLbVCFD7mUbBVZZJ3myg0WAOC8kTKALtiD5A7JNhYyIbMC19Eat93xLaYeCwmgDGVSrc6KRY0SPlVK1L9QLPwSb%2bnC0XB6spyuaqh1gkNMbM88x2pVDKEMC7hw1GjFxvf1uPAsEtgUlkavHJPc2kBtDoDqBqX6f3JbKmDCqtTkFy/qEslMdkjoiYDYlaLeB6v9atF0UIUW7vnpybWAurYcnAUlzxwEJc9dAQfeOAQNuwbuqCq9fIrQ0pRU8KCfK5tNVbW%2b621vCYyUyOopCOKUuS7Y1NeWNZdA9Offw6TFguhswuT9gkYDBxGRoSJTxhN4zhwegAmUnslFeeQOQIfgWgCn9p4DB999CQ%2b8dhxXPHoCVz1yG9gcweQz6QxE4kKwZRKJYmsxRGPJxEjdclCiusQUfo9q9mxtqI02C5UaiKeuRIhz7JspDl6aWLD7yG4534MDJsx2N%2bPSYcXRULmgTtvg/0/7yc0nyFUnkGEPsPhKEJEi31koBs27oHuji7c8cx%2bWPTnMDLC9FZPn6N06mEyjuBgdx%2buuPdVXPnwYVz92Al8fMMJXPnDI%2bgdNMA7aUd//3kEgyFM2G2w261wuyn1OT1k6DFYx61iM4bPF1idARZynSyBWl3jsqwNeMNSxtWPuN%2bGVDojuEOxUEKASFHf4CCylepiPombfn4Guv/4Le7p6sGk2QTDiAlDpBcmyFs8bieGiULbbBbcvXU3dP/%2bKi5bfwy6%2bw/iW786e5FDQLORSRggTa517lXkpy2CCrMLsyRVCpqaNfsTx48jGY%2bjQmDFNFod5/JOLl80hd1nJ5BI55bsWIaAdMthE27f0Y8n3jUimy9KhGr5qs%2bqqXBn/bwhkk96z7%2bhRtQ39%2bRH4DUPIFeoCHbIaY7rfYwHOSnW/BST/QMDyqA5g3Rsg9N8Z0Rv1OczQstQNZE9Koph6x33N5f4XK0gWtgAXAqnCcv97FpRI%2bC1ANvJnRgctcI0NgajYRRn%2b/oxcG5QZII4uXLfxkcQJhrK%2bZsXU5KJhKgDMo/nCi3nb3kTAyO9mu%2b3Vp8pPdZbKZbBty6utYRTrdHslNMXc6usnAmyzj6ku27A3PFNgiJryU9VmiXX/2zF6Cc/jNFTx2G0jkM/PIwJiwkjw3qcOTuI86PnYTEaFPfVVonk7bOCR3D5utIqubXvIO1cvr94m6VVabGqYptqUdJGfuZSmDGOiho8c3R5tyjvDSzLfEG1l3je3esK4xt1z2D7CTPS2fzSKvFib5aWq6tNuQIkCg719q0tZARZjyvcXV7vZ74vafcsAaGv9xSq2cxy2y1x8/M90N21DzsODsDjsMNiHofVYsPYuAWDQ%2bco3U2i0bzwTVGrSoNt6Kp5sazW5GuM/LyE3pRqCHx4Dx2A/jIdLJsfhc3nI1LkpFQ3BYfbBTuxRd5v7HC4hVsfGp3C/bsG4Z%2bJUabJIJ/Li5MJTSQaQVyqMb4vGLDcpkk1L6jKCM6xLIcKeUievCBCRMX2g/sQ7O1GlFhbjBheLBZHjAYTiRBJCodJayQ6ssNab4e9YAMs5hmK2mOVRvwgeOoEMqTDeRm7tMyiaKcuaEqo31xW3LzvHrDkBkgJC2IEgkN/%2bkcwvbkTJpLOllEjzg8OwTCqx7n%2bERiN1lZ1uYklB/l%2b/9cZ3Xu9UdvRGuX6JMV1kXI/L5GzdGVxkqF4znBhhZjd7%2br/BV0UA7zX2Pz/NHg%2b/g/0vYAfFRzH9wAAAABJRU5ErkJggg==' /%3e%3c/svg%3e" width="669" data-srcset="/assets/static/word2viz-tagoverflow-english.58cb598.db1fdc0c46c48f025af28d46ded455cf.png 669w" data-sizes="(max-width: 669px) 100vw, 669px" data-src="/assets/static/word2viz-tagoverflow-english.58cb598.db1fdc0c46c48f025af28d46ded455cf.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/assets/static/word2viz-tagoverflow-english.58cb598.db1fdc0c46c48f025af28d46ded455cf.png" width="669"></noscript></a></p>
</div><h2 id="pointwise-mutual-information-and-compression"><a href="#pointwise-mutual-information-and-compression" aria-hidden="true"><span class="icon icon-link"></span></a>Pointwise mutual information and compression</h2>
<blockquote>
<p>The Compressors: View cognition as compression. Compressed sensing, approximate matrix factorization - <a href="https://news.ycombinator.com/item?id=10954508" target="_blank" rel="nofollow noopener noreferrer">The (n) Cultures of Machine Learning - HN discussion</a></p>
</blockquote>
<p>In principle, we can compute <em>P(a|b)</em> for every word pair.
But even with a small dictionary of 100 000 words (bear in mind that we need to keep all declinations, proper names and things which are not in official dictionaries, yet are in use) keeping track of all pairs would require 8 gigabytes of space.</p>
<p>Often instead of working with conditional probabilities, we use the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" target="_blank" rel="nofollow noopener noreferrer">pointwise mutual information</a> (PMI), defined as:</p>
<p>$$ PMI(a, b) = \log \left[ \frac{P(a,b)}{P(a)P(b)} \right] = \log \left[ \frac{P(a|b)}{P(a)} \right].$$</p>
<p>Its direct interpretation is how much more likely we get a pair than if it were <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)" target="_blank" rel="nofollow noopener noreferrer">at random</a>.
The logarithm makes it easier to work with <a href="https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/" target="_blank" rel="nofollow noopener noreferrer">words appearing at frequencies</a> of different orders of magnitude.
We can approximate PMI as a scalar product:</p>
<p>$$ PMI(a, b) = \vec{v}_a \cdot \vec{v}_b, $$</p>
<p>where $$\vec{v}_i$$ are vectors, typically of 50-300 dimensions.</p>
<p>At the first glance it may be strange that all words can be compressed to a space of much smaller dimensionality. But there are words that can be trivially interchanged (e.g. <em>John</em> to <em>Peter</em>) and there is a lot of structure in general.</p>
<p>The fact that this compression is lossy may give it an advantage, as it can discover patterns rather than only memorize each pair.
For example, in recommendation systems for movie ratings, each rating is approximated by a scalar product of two vectors - a movie's content and a user's preference. This is used to predict scores for as yet unseen movies, see <a href="http://katbailey.github.io/post/matrix-factorization-with-tensorflow/" target="_blank" rel="nofollow noopener noreferrer">Matrix Factorization with TensorFlow - Katherine Bailey</a>.</p>
<h2 id="word-similarity-and-vector-closeness"><a href="#word-similarity-and-vector-closeness" aria-hidden="true"><span class="icon icon-link"></span></a>Word similarity and vector closeness</h2>
<blockquote>
<p>Stock Market ≈ Thermometer - <a href="http://byterot.blogspot.com/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html" target="_blank" rel="nofollow noopener noreferrer">Five crazy abstractions my word2vec model just did</a></p>
</blockquote>
<p>Let us start with the simple stuff - showing word similarity in a vector space.
The condition that $$P(w \vert a)=P(w \vert b)$$ is equivalent to</p>
<p>$$ PMI(w, a) = PMI(w, b), $$</p>
<p>by dividing both sides by <em>P(w)</em> and taking their logarithms.
After expressing PMI with vector products, we get</p>
<p>$$ \vec{v}_w \cdot \vec{v}_a = \vec{v}_w \cdot \vec{v}_b $$</p>
<p>$$ \vec{v}_w \cdot \left( \vec{v}_a - \vec{v}_b \right) = 0 $$</p>
<p>If it needs to work for every $$ \vec{v}_w $$, then</p>
<p>$$ \vec{v}_a = \vec{v}_b. $$</p>
<p>Of course, in every practical case we won't get an exact equality, just words being close to each other. Words close in this space are often synonyms (e.g. <em>happy</em> and <em>delighted</em>), antonyms (e.g. <em>good</em> and <em>evil</em>) or other easily interchangeable words (e.g. <em>yellow</em> and <em>blue</em>).
In particular most of <a href="https://en.wikipedia.org/wiki/Horseshoe_theory" target="_blank" rel="nofollow noopener noreferrer">opposing ideas</a> (e.g. <em>religion</em> and <em>atheism</em>) will have similar context. If you want to play with that, look at this <a href="https://demos.explosion.ai/sense2vec/?word=machine%20learning&#x26;sense=auto" target="_blank" rel="nofollow noopener noreferrer">word2sense phrase search</a>.</p>
<p>What I find much more interesting is that words form a linear space. In particular, a zero vector represent a totally uncharacteristic word, occurring with every other word at the random chance level (as its scalar product with every word is zero, so is its PMI).</p>
<p>It is one of the reasons why for vector similarity people often use cosine distance, i.e.</p>
<p>$$ \frac{\vec{v}_a \cdot \vec{v}_b}{\vert \vec{v}_a \vert \vert \vec{v}_b \vert}. $$</p>
<p>That is, it puts emphasis on the direction in which a given word co-occurs with other words, rather than the strength of this effect.</p>
<h2 id="analogies-and-linear-space"><a href="#analogies-and-linear-space" aria-hidden="true"><span class="icon icon-link"></span></a>Analogies and linear space</h2>
<p>If we want to make word analogies (<em>a is to b is as A is to B</em>), one may argue that in can be expressed as an equality of conditional probability ratios</p>
<p>$$ \frac{P(w|a)}{P(w|b)} = \frac{P(w|A)}{P(w|B)} $$</p>
<p>for every word <em>w</em>. Sure, it looks (and is!) a questionable assumption, but it is still the best thing we can do with conditional probability. I will not try defending this formulation - I will show its equivalent formulations.</p>
<p>For example, if we want to say <em>dog is to puppy as cat is to kitten</em>, we expect that if e.g. word <em>nice</em> co-occurs with both <em>dog</em> and <em>cat</em> (likely with different frequencies), then it co-occurs with <em>puppy</em> and <em>kitten</em> by the same factor.
It appears it is true, with the factor of two favoring the cubs - compare <a href="https://books.google.com/ngrams/graph?content=nice+cat%2Cnice+kitten%2Cnice+dog%2Cnice+puppy&#x26;year_start=1960&#x26;year_end=2008&#x26;corpus=0&#x26;smoothing=5&#x26;share=&#x26;direct_url=t1%3B%2Cnice%20cat%3B%2Cc0%3B.t1%3B%2Cnice%20kitten%3B%2Cc0%3B.t1%3B%2Cnice%20dog%3B%2Cc0%3B.t1%3B%2Cnice%20puppy%3B%2Cc0" target="_blank" rel="nofollow noopener noreferrer">pairs</a> to <a href="https://books.google.com/ngrams/graph?content=cat%2Ckitten%2Cdog%2Cpuppy&#x26;year_start=1960&#x26;year_end=2008&#x26;corpus=0&#x26;smoothing=5&#x26;share=&#x26;direct_url=t1%3B%2Ccat%3B%2Cc0%3B.t1%3B%2Ckitten%3B%2Cc0%3B.t1%3B%2Cdog%3B%2Cc0%3B.t1%3B%2Cpuppy%3B%2Cc0" target="_blank" rel="nofollow noopener noreferrer">words</a> from <a href="https://books.google.com/ngrams" target="_blank" rel="nofollow noopener noreferrer">Google Books Ngram Viewer</a> (while n-grams look only at adjacent words, they can be some sort of approximation).</p>
<p>By proposing ratios for word analogies we implicitly assume that the probabilities of words can be factorized with respect to different dimensions of a word. For the discussed case it would be:</p>
<p>$$
P(w\vert dog) = f(w\vert species=dog) \times f(w\vert age=adult) \times P(w\vert is_a_pet) \
P(w\vert puppy) = f(w\vert species=dog) \times f(w\vert age=cub) \times P(w\vert is_a_pet) \
P(w\vert cat) = f(w\vert species=cat) \times f(w\vert age=adult) \times P(w\vert is_a_pet) \
P(w\vert kitten) = f(w\vert species=cat) \times f(w\vert age=cub) \times P(w\vert is_a_pet) $$</p>
<p>So, in particular:
$$</p>
<p>How does the equality of conditional probability ratios translate to the word vectors?
If we express it as mutual information (again, <em>P(w)</em> and logarithms) we get</p>
<p>$$ \vec{v}_w \cdot \vec{v}_a - \vec{v}_w \cdot \vec{v}_b = \vec{v}_w \cdot \vec{v}_A - \vec{v}_w \cdot \vec{v}_B, $$</p>
<p>which is the same as</p>
<p>$$ \vec{v}_w \cdot \left( \vec{v}_a - \vec{v}_b - \vec{v}_A + \vec{v}_B \right) = 0.$$</p>
<p>Again, if we want it to hold for any word <em>w</em>, this vector difference needs to be zero.</p>
<p>We can use analogies for meaning (e.g. changing gender with vectors), grammar (e.g. changing tenses) or other analogies (e.g. cities into their zip codes).
It seems that analogies are not only a computational trick - we may actually use them to think all the time, see:</p>
<ul>
<li>George Lakoff, Mark Johnson, <a href="https://www.amazon.com/Metaphors-We-Live-George-Lakoff/dp/0226468011" target="_blank" rel="nofollow noopener noreferrer">Metaphors We Live By</a> (1980)</li>
<li>and <a href="http://web.archive.org/web/20080718021721/http://cogsci.berkeley.edu/lakoff/metaphors/" target="_blank" rel="nofollow noopener noreferrer">their list of conceptual metaphors in English (webarchive)</a>, in particular look for <em>X is Up</em>, plotted below:</li>
</ul>
<div class="images">
<img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 686 483' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ba70d90e29b149d8443802a9025ec2ec'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ba70d90e29b149d8443802a9025ec2ec)' width='686' height='483' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAtCAYAAAAeA21aAAAACXBIWXMAAAsSAAALEgHS3X78AAAKVElEQVRo3t1a%2b1faWhb2z5%2b15pf5ae7MWnNXO71d7b297e2q1vpoEUXFolZQi3pBBIHwJrxJQgjf7H2SQBICWq99OHGdlRA8h7Nf3/72ThbgOEajke/1//OxMBzqCLxfQ7laFzeGQ4PG8EEqwd4vn71jpgIwMrC28hZSsSJuGIaBZrP5sBQwFpLP5r5ZDlt4%2b5rP5pgoZcG5jpS7xud4Es3WA1GAKa1HCV/mMQvOiYqi0FDR6XR%2bXAXYQltj7ObW12RglCicT%2bJx1Gp1NBpNXF1dIS/lkc1mkUmn6ZzHYDCwPcAbJ6MfUwEOSzstP941YRnUHlQK3%2bRFAsF3S8gkkzg/TyIQ3MLKygoikQMsLS5idy8CTVMnIeAEDwbAH0YBHhd3fbbvaQpGXQrZdg1Gs0pKoL0P%2bl8QAj5p77srwE9w5zEk9%2b21ABJ41CjRuQJFriIUDGJ5ZQ1Lb97g5evXyOSKY2Af0hgMNOi6Pg2C9odSUUIyefnts4BT2JnWpuDudzAiYVGXMJJJ8BYpoNsg8OpiSIJ1ez0USyXIcoPiv4pOs4Fuq4sPHzYRWF/Bu9U1NByy8Z9LAe12C7V6/dt4gFdIG4csQBsPimsWelTNATXJVAC5/Eih%2b7pmpTiysktZxGO0nsAEg8Aun5cIDGXKbi3Bc5yyfdsQ8CD4TEAjV2XLjtjSlSxZm1yZLU0CQR/M2Rfd77dh0GBQHN1AmL4NCM4CMq/QJBg6JHQtT5am0SJLK20y7cB0f4dXiLjWDcRiJ/j50WP89M%2bf8Oy/j3FxHEWv25v875gITbPCr%2b8Bzh9y5GrnMEhog9zZIIGNUhpGJUcuXkWvWsKHwBbF7jrF7TpCO2HCPU14gAgJjv1qHo3UOU4PD/GemOxrAr0PGwFUarJ7734g6qLCjn%2buVctIZ67vBoI%2blh45rCwGxyu7JoEZIzdaZQIeOg%2b69C3lcYNys0aApvQhFwtoSBmUMym0SnmaQ249UGHUi6QwCUaPvUO/wcVv3r9LAXK9BqlQ/DIPmIfcQmgCJAIr6KqZo5uE3rUcCVJCMnaA4/2POP64g09b7xELBXEROyQyU%2bNc5xZAhAjleb0/JSyHrc3zvS5%2bKwXcJgSmKy23e08phcGK0hNbDmSts497WHn2CDtLL9EuXEOvVxE/3MfncBAHYVLApyMcHhziKBpFX9XMOLewQSA/ZYFsOoNPsRi2t0MI7W4jSteqpk1VgF%2bsgJtA0LuwS9O2ncjS7KImQaENN8oEaJJAcs7XtUwSV6kkysUiBrWCuGeKOMOxmN1x%2bitnSPgsRYeK5OU1zuKnODr6hNjJCRVuZ9AsTn9XvLqFB7hLTOdhEJ82WjViY2UzR5OVhMCctlgJ7AHG0P2LSpPNOlnDQmmD3bjbgkEER2QCFp6VyGvcsg9wZwXYC7SIOcn1hgsEza8mP1Ap1xBYfI299ytkbULcIcVkp45Rx2RkzM3hgSBFM5A6jyO0%2bDu2t0IIhnYR2tpCKvGnwAZG/lG9INidIDisaBbcyuVO957U9HeL%2bbkKqJRLqFIpyYzQ6QFMnqR8AYloBNvr61h69YJ49wYO9nYRIeDKZTIOeompHK/pI0jJC6TPT3F1FsfzX57iH3//G33%2bTEpsCstzrBsiNepmahzqvkjuxaW/ylVuDgFLqFwuj/DuHn7%2b178R2Sa0Jose7O9TifkRhVJ5iqiABWBGxylPLpikRifvGPQEudFbddJM3yzgJ1UO00DHRx2J5DXerbzD4vIycXz53hnqjSBoOsHkB6vlmn8cssC6JTAB4YgHh0RHNpGcvjcorjl/i/s2MWK3potiJo3E51NcnJ3hijwjnzgXClLUAa6vM0imrr5Kr/JWaXCmq3Ex0uuYFRkTFR6sBBZW%2bL5i8nq%2bzyDJFncwQvtQFR1RSmuBtWVsrK4isLqMyP7hvQPejR7Q63aoGmyg1WpNCc7lZqNaIXAmnMhcop7LoE2UFZT6BMPzRitVY6N2zQRHpq4eAYSntZrQmnWBAxhqlvJVhPciZPGUK%2b3eB%2bDdqICClMNFIuUCQfs7qSgjsLyI50%2be4sWzZ1h%2b%2b060ltyAZMZz8nMc63/8hpOjGJm3O6HCjvWKZRnrb98g8GEDVoVKVVwHQ7U/Trtegb337kp%2b7hQC4zRuMS/XPOcmSAC5WkO5WoXWkl3/411zQF41jmsucrT%2bVId35GmAzur%2b3jkNejU4DYLzte8V3uYBrUIW%2bVwW1Xrdd75r8zxH6fmvPV3xuw6NQFL/C%2bC44DfRq4CbFGEK0RckRlSVjR4iuzvY2Azi7PxPF0eYKo%2bZPnOra1btxv/PPUAmRewhTLWJFseJBr9ZfIvfX7ykVJka9/7u5AH2BlVVgdJXXCB4Y9lrob3o1syY43JjhyUFTVY6Vi1hCN4gqj7VEnQ82mZq5ftWSlUVBXKjiX6/P9XmukMImB%2bylIulfJEo8Yx%2bgF8scz1AFmROPzIMF3/wWtVVNLHSmmUzhdqCMofg6pEtzW7N3MIbLvdcD/iGQLfbnV7Ux/JCEKXr/4CSBBBhIeK7KxqZopnJglKK5KbGoNs2iy0Wdp4AHhDEjIefd/YAp5tyHE31A3yF14T7svelL5PIJBPIpS5Nl%2bbeHrsthwULz1xBeIgJVpVKHS9%2b/Q2las3/d3x6hze1tu6FB9jXvgpwnm3KKwod4OhgH8HVFcSOomMw8kPvcXWoaqL9bjhC5msLeisP4BTIDw3b7bY/4tsta%2bbzM569j%2bZ0gH1T4Hd%2b/LZgFiPmJjJXlzil9OJtirqajLoyM04NFwhOC%2brLI76zElzvB7AATCqcIGhvrt9XIUtZFItFlGgUChJdl1CUiqgS6%2bPH6g/inQKvArgVJUl5QUv9MGDC3ZtU%2b%2b8jFNrEZnADm1ub2AjQObhJfD5AMS37kqcfdYyfDusU00tLb9DpKWMvmFcLfK3e3P08efuyfQoF8NOZP169hNzsjBXgBUEbJ5xlqbcv95CGiWYmU1tQel388uQxFS2tcZ1eqVSEF/BgWtxoNMQ1K4bP/Jnv2//D9xuyPP5sz2Mw9c5rO%2bbxkH3m2b/nWt%2bzB17bd33rejJPdv0PPzpvNSdUX2QBWa67MMB5SNlrnMfjqFiv0TEHv0olcHHBPXlzTrsp4zByQIVPYjyvXMghvLMrANJsqAxxGo3i%2bPiEyJP1OL7ZwPlpHOl0YTyvVMyLQqdmkSTGqETiAjuRw3HpzEY7PIiKjGUftXoVO9shVCvVsRzJyyuEd8PEVcx9Doh/RGPHtM%2bL6TdEnMKzkCpVXBrV/p1Ol%2bqPScnJLj%2bg%2b/ydnfZ4k30qolRHr0Cn0Or1%2buKNDHtt8RKWqrq6QhptajCYPCdgHsJrO/t/vBde355nGENRBKmq8/d0%2br0enZ3zNDHPsFyf1223m8JTDMsIC96eOx/pdApPH/0HT58%2bwbVUmdsXnJfv54HTt54XDofx/NkveP3qV6yurkE3TCRYmIWOCmmYLTarHzCP2MwiO99rnuhmUyiy5fk9oYHjcdrCbVLJQ39veJ5M/wMHBBdCcy340wAAAABJRU5ErkJggg==' /%3e%3c/svg%3e" width="686" data-srcset="/assets/static/word2viz-up-down-metaphors.f451e11.92b4f3cf64f7ab13e3d9493b58e170e3.png 686w" data-sizes="(max-width: 686px) 100vw, 686px" data-src="/assets/static/word2viz-up-down-metaphors.f451e11.92b4f3cf64f7ab13e3d9493b58e170e3.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/assets/static/word2viz-up-down-metaphors.f451e11.92b4f3cf64f7ab13e3d9493b58e170e3.png" width="686"></noscript>
</div><h2 id="differences-and-projections"><a href="#differences-and-projections" aria-hidden="true"><span class="icon icon-link"></span></a>Differences and projections</h2>
<blockquote>
<p><code>woman - man = female - male = she - he</code>, so <code>wo = fe = s</code> (a joke)</p>
</blockquote>
<p>Difference of words vectors, like</p>
<p>$$ \vec{v}<em>{she} - \vec{v}</em> $$</p>
<p>are not words vectors by themselves.
However, it is interesting to project a word on this axis.
We can see that the projection</p>
<p>$$
= \log\left[ P(w|a) \right] - \log\left[ P(w|b) \right]$$</p>
<p>is exactly a relative occurrence of a word within different contexts.</p>
<p>Bear in mind that when we want to look at common aspects of a word it is more natural to average two vectors rather than take their sum. While people use it interchangeably, it only works because cosine distance ignores the absolute vector length. So, for a gender neutral pronoun use $$ (\vec{v}<em>{she} + \vec{v}</em>)/2 $$ rather than their sum.</p>
<p>Just looking at the word co-locations can give interesting results, look at these artistic projects - <a href="http://www.chrisharrison.net/index.php/Visualizations/WordSpectrum" target="_blank" rel="nofollow noopener noreferrer">Word Spectrum</a> and <a href="http://www.chrisharrison.net/index.php/Visualizations/WordAssociations" target="_blank" rel="nofollow noopener noreferrer">Word Associations</a> from Visualizing Google's Bi-Gram Data by <a href="http://www.chrisharrison.net/" target="_blank" rel="nofollow noopener noreferrer">Chris Harrison</a>.</p>
<h2 id="i-want-to-play"><a href="#i-want-to-play" aria-hidden="true"><span class="icon icon-link"></span></a>I want to play!</h2>
<p>If you want <strong>explore</strong>, there is <a href="https://lamyiowce.github.io/word2viz/" target="_blank" rel="nofollow noopener noreferrer">Word2viz</a> by Julia Bazińska.
You can choose between one of several pre-defined plots, or create one from scratch (choosing words and projections).
I've just realized that Google Research released a tool for that as well:
<a href="https://research.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html" target="_blank" rel="nofollow noopener noreferrer">Open sourcing the Embedding Projector: a tool for visualizing high dimensional data - Google Research blog</a> (and the actual live demo: <a href="http://projector.tensorflow.org/" target="_blank" rel="nofollow noopener noreferrer">Embedding Projector</a>).</p>
<p>If you want to use <strong>pre-trained vectors</strong>, see <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="nofollow noopener noreferrer">Stanford GloVe</a> or <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="nofollow noopener noreferrer">Google word2vec</a> download sections.
Some examples in Jupyter Notebooks are in our playground <a href="https://github.com/lamyiowce/word2viz" target="_blank" rel="nofollow noopener noreferrer">github.com/lamyiowce/word2viz</a> (warning: raw state, look at them at your own risk).</p>
<p>If you want to train it on your <strong>own dataset</strong>, use a Python library <a href="https://radimrehurek.com/gensim/" target="_blank" rel="nofollow noopener noreferrer">gensim: Topic modelling for humans</a>.
Often some preprocessing is needed, especially look at <a href="https://explosion.ai/blog/sense2vec-with-spacy" target="_blank" rel="nofollow noopener noreferrer">Sense2vec with spaCy and Gensim</a> by Matthew Honnibal.</p>
<p>If you want to create it <strong>from scratch</strong>, the most convenient way is to start with <a href="https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html" target="_blank" rel="nofollow noopener noreferrer">Vector Representations of Words - TensorFlow Tutorial</a> (see also a respective <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb" target="_blank" rel="nofollow noopener noreferrer">Jupyter Notebook from Udacity Course</a>).</p>
<p>If you want to <strong>learn</strong> how it works, I recommend the following materials:</p>
<ul>
<li><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/" target="_blank" rel="nofollow noopener noreferrer">A Word is Worth a Thousand Vectors</a> by Chris Moody</li>
<li>
<p>Daniel Jurafsky, James H. Martin, <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="nofollow noopener noreferrer">Speech and Language Processing</a> (2015):</p>
<ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/15.pdf" target="_blank" rel="nofollow noopener noreferrer">Chapter 15: Vector Semantics</a> (and <a href="https://web.stanford.edu/~jurafsky/slp3/slides/vector1.pdf" target="_blank" rel="nofollow noopener noreferrer">slides</a>)</li>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/16.pdf" target="_blank" rel="nofollow noopener noreferrer">Chapter 16: Semantics with Dense Vectors</a> (and <a href="https://web.stanford.edu/~jurafsky/slp3/slides/vector2.pdf" target="_blank" rel="nofollow noopener noreferrer">slides</a>)</li>
</ul>
</li>
<li><a href="http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf" target="_blank" rel="nofollow noopener noreferrer">Distributional approaches to word meanings</a> from <a href="http://web.stanford.edu/class/linguist236/" target="_blank" rel="nofollow noopener noreferrer">Seminar: Representations of Meaning course at Stanford by Noah D. Goodman and Christopher Potts</a></li>
<li>
<p><a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="nofollow noopener noreferrer">GloVe: Global Vectors for Word Representation</a> and paper</p>
<ul>
<li>Jeffrey Pennington, Richard Socher, Christopher D. Manning, <a href="http://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="nofollow noopener noreferrer">GloVe: Global Vectors for Word Representation</a> (2014)</li>
<li>it's great, except for its claims for greatness, see: <a href="https://rare-technologies.com/making-sense-of-word2vec" target="_blank" rel="nofollow noopener noreferrer">GloVe vs word2vec</a> by Radim Rehurek</li>
</ul>
</li>
<li><a href="http://norvig.com/chomsky.html" target="_blank" rel="nofollow noopener noreferrer">On Chomsky and the Two Cultures of Statistical Learning</a> by Peter Norvig</li>
</ul>
<div class="images">
<img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 400 600' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-286494be316e7ff6f97306fcf1f9a8e2'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-286494be316e7ff6f97306fcf1f9a8e2)' width='400' height='600' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCABgAEADASIAAhEBAxEB/8QAGwAAAgMBAQEAAAAAAAAAAAAABQYDBAcBAgD/xAA1EAACAQIEBAMGBQQDAAAAAAABAgMEEQAFEiEGEzFBIlFhFCMycZGhFTOBweEHQrHwJFLR/8QAGgEAAgMBAQAAAAAAAAAAAAAAAwUCBAYAAf/EACQRAAICAgICAgIDAAAAAAAAAAECAAMREiExBBMiYQVBI1Fx/9oADAMBAAIRAxEAPwBOzGasny6KNTUUyPdgtgol6i5Pc32vgFlYpqmCNnhYyqSHbUW132tvsB9MMWbpHIJKTmfBGxYG9xY7C9rb3NgPW%2bFuhqky%2bB1hiluWGoC4BPkx64U1JY64Y/5zIDAk70c1NmkccqNGQ3J0IbEC3lbrex9cG5Kl2pZBNFJU1040xsNyzjawUDr0wOyeeaszoyy8qCVLu8ixhmA8wT1OCOWZlVQzqtdKyyLqMUyOFKA%2bWx3P13wLyVQEAnr6k1OD1A9Nk0lXTTDM44o501Dxgh1I27DA2DLHpaqWGGtRPdli24DjuPQ9Ou2HSTLaqrh1SSukS3Kv8Lte1rgX74F8SU1OKmOeCBmKKsf5hIDW3%2bp7HAqmddixwP6nrKw5IgenPs8kkc9OgZBsH7E/3A4tSyQiqaedN2VnKpZQdx08uvQYhqqXUkakkoSCxv07kX7YrVMTSFRI6GQL7kqOgHU7/pgWAW74gx39TQa4SV2dU9FROiVFZIAjAfli7am9NlwF4xovwySKgoghmVeqG41dzfzw4ZzD7DxLl5j1FkkkOq/i1aOv3wCrVaesqKiUk%2b6IW69Texw%2bavoQAYAiJTxSRVkcSVKXjALmIaQegIB/ycNPD%2bWPLOHnDuly6CVr7g7fscA6zK3jmaJXulKjNIeg%2bIDb5sQMPGR0zKIFayyObWve3pijbWdgTLlGHbEipaOrmM00sqRQ6iqi51Nbv5Yrzohf2SVGExGpGuGDeoOCktLUvmNTAK%2bP2eJNlC20v6nvgKy1slLRVVU9Mz8xkQwk7i3fA7FwsZtWNeoHqG5VI9CwVTI5cuy7i/X5/wA4HRe8IRtDsi%2bEW3Ve%2b/zwx5lEkTGWbTqlUXPxFVuLm3nttgRGOcizTwHlavEQ1mZLnYHtfbC0Njj7ihlwcGaDxKS/F1I0l95pLldvhW22PE1CJKSIRhi8rlVF/OQfvgtVU0E3Ekk9UXQ03NkRVANyxtcjfp1x54eqqN%2bJ4y5lkZIyV8AsDe4Jt0PXtjVewbhJBfGZ6/ZOVXAEyy1UklZDy5CLKkZZratW9zbHqClGT5nRxUssNRG0JLtIgYqbkWAvsQLb4d5KnV0wNzClp6yPTKtiDqDJ4WB%2beOu8XflYSh1RssIi51R001TCql4y762C/wBxwOyOlbMM0zGho4QrZcQdLN1D3Or7Ya5%2bH2FQJUnLgdL7EYKZBFRUDVAjBWsq3VpHkABk0C1gOtgL/XC%2byhs6vGT3KyfE5md5hl1TQyilqo/ZI1jMg5qbyEg%2bFSOoJ2wFaT3bokEaSatQA2QDfYDy6Y0fjeCeGGrklJliTSYzf8u9ltb5thBrlWKOKq0gvosoJNrDbf74RX0%2bpyoEX2LrjBjB/UCpNLVSMxtBUStGsnbYhyD6emGjhjL46XK6ZxTpDJImsqBYjVvY/XFbOMobPKumjzCnhdIKszMwc3Xa%2bm3e4tthiLY2NKZcuZHf%2bIKJVpq4TtpVHjGoqC6EaiOtv93x6qUqQJZIJy0hVQkbWCAg7m9r7/tj25I3B3/zj4PcXGLOIPJ5E5A85OmZUIt%2bYpt%2bhHY47MiM4YqNYBUNbcA9QDjjNbxDbzxwPcagQQfLESAe54OJWzuOWuyquiQ/8mWJEhJNgrK4a5%2bmEg0lTVTU1LmdMsDmYRjUw94Wvdttt8PFVUJBBJLKdMcalmPkALnCVmHPrOVVPLoMpMijoVIOygjyFsZ/8zVUgV/3CAluDKuZ5lNUqRJJy3ZtbtHdLOBsR%2bhtg5w9m2Ymop4MwmjnjmHhkK6WU3It64V84Ke1xFXYoI3JX/sbDbB7KlaorMnAZ1WCp1NYbHcGx9Bv9MNs6HaCQswCiN/OVmIDA2Yr9CQfuDjsmtVJiALdwTb/AE4F8Vw1OUxUkFJIEibmOZNGqxLarfc4G5NnU82pXLMFW5YrYEfsfTHL5a7aMOZYak67CMTpHUQOk8ZZGBVklGxHy6YimiQHVEzQvtunewsAR0OBy8QZe9S1PJULFKpsQ%2by3te1%2bmLVVMscLyMToVSxI32AucWA6tyDAsDjDQLxJnMgJpKeBjf4mcbEfob2wvVddXrNWxxRxFGC6dVwImCeILtcfz1wFrs2/EKt62GBWaVBCS5BVACSPnsTiaorZ6R0qYgGilPiDKFSNShs5t0At2xnPOs9zDXkmW2ZNFC9wlLNCc7UxMlTBGjNZOhAAuMNmRRU9bmklBT2SQTRjQjbxlhqN7emMxy0Ll2bTxSVIZEOl3OwJa2oX8vXDTw3m9DwvnlXmQvUoWRpN2ZgBdbKSeu99%2bvphuAbWCr2cQSAVoWP6mrcT%2bB1LyRxi1mHZv/DjO84zB6etihgpJJYJDZyo2UdNv39MScS8S8MZ/LzI6znyFfhdWVh6W/jAuj5lKAIUNRlrjVGwcNo%2b97emBWFlcqRiEGCoYGe6Khy9sx5U8gMjHUAxvptuAB2GH7LoaaiopBp5zPG0cd12JYWPrsL4z3h4034rriillJJKl1GlTt6k/bDunFWSZJIZcyzCCepjBMdNG2ttRFtgOnzNsQrRrDisc/U5mVOXPEyCGijgaSkaWQLGCEGkKQ3f9MTwznmSxlTNThxDytI1aT3Uny229MS5RTz1WZ1dRLSVU4hVpF3ADA79TboPLriPJ/Zq2hmqOYYAJNSq4Nyf9vhRd8WYHsdwXZyOjP/Z' /%3e%3c/svg%3e" width="400" data-srcset="/assets/static/queen-julia-vectors.d042d20.031397cd7991b188254198da1b5223cc.jpg 400w" data-sizes="(max-width: 400px) 100vw, 400px" data-src="/assets/static/queen-julia-vectors.d042d20.031397cd7991b188254198da1b5223cc.jpg"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/assets/static/queen-julia-vectors.d042d20.031397cd7991b188254198da1b5223cc.jpg" width="400"></noscript>
<blockquote>
</div><p>Julia Bazińska, at the rooftop garden of the <a href="https://en.wikipedia.org/wiki/Warsaw_University_Library" target="_blank" rel="nofollow noopener noreferrer">Warsaw University Library</a> - the building in which we worked</p>
</blockquote>
<h2 id="technicalities"><a href="#technicalities" aria-hidden="true"><span class="icon icon-link"></span></a>Technicalities</h2>
<p>I tried to give some insight into algorithms transforming words into vectors. Every practical approach needs a bit more tweaking. Here are a few clarifications:</p>
<ul>
<li>
<p>word2vec is not a single task or algorithm; popular ones are:</p>
<ul>
<li>Skip-Gram Negative-Sampling (implicit compression of PMI),</li>
<li>Skip-Gram Noise-Contrastive Training (implicit compression of conditional probability),</li>
<li>GloVe (explicit compression of co-occurrences),</li>
</ul>
</li>
<li>while <em>word</em> and <em>context</em> are essentially the same thing (both are words), they are being probed and treated differently (to account for different word frequencies),</li>
<li>there are two sets of vectors (each word has two vectors, one for word and the other - for context),</li>
<li>as any practical dataset of occurrences would contain PMI $$-\infty$$ for some pairs, in most cases positive pointwise mutual information (PPMI) is being used instead,</li>
<li>often pre-processing is needed (e.g. to catch phrases like <em>machine learning</em> or to distinguish words with two separate meanings),</li>
<li>linear space of meaning is a disputed concept,</li>
<li>all results are a function of the data we used to feed our algorithm, not objective truth; so it is easy to get stereotypes like <code>doctor - man + woman = nurse</code>.</li>
</ul>
<p>For further reading I recommend:</p>
<ul>
<li><a href="https://www.quora.com/How-does-word2vec-work" target="_blank" rel="nofollow noopener noreferrer">How does word2vec work?</a> by Omer Levy</li>
<li>Omer Levy, Yoav Goldberg, <a href="https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf" target="_blank" rel="nofollow noopener noreferrer">Neural Word Embeddings as Implicit Matrix Factorization</a>, NIPS 2014</li>
<li>with a caveat: <a href="http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/" target="_blank" rel="nofollow noopener noreferrer">Skipgram isn’t Matrix Factorisation</a> by Benjamin Wilson</li>
<li><a href="http://nlpers.blogspot.ie/2016/06/language-bias-and-black-sheep.html" target="_blank" rel="nofollow noopener noreferrer">Language bias and black sheep</a></li>
<li><a href="https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/" target="_blank" rel="nofollow noopener noreferrer">Language necessarily contains human biases, and so will machines trained on language corpora</a> by Arvind Narayanan</li>
<li><a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/" target="_blank" rel="nofollow noopener noreferrer">Word Embeddings: Explaining their properties</a> - on word analogies by Sanjeev Arora</li>
<li>Tal Linzen, <a href="https://arxiv.org/abs/1606.07736" target="_blank" rel="nofollow noopener noreferrer">Issues in evaluating semantic spaces using word analogies</a>, arXiv:1606.07736</li>
<li><strong>EDIT</strong> (Feb 2018): Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney, <a href="http://www.aclweb.org/anthology/P/P17/P17-1007.pdf" target="_blank" rel="nofollow noopener noreferrer">Skip-Gram – Zipf + Uniform = Vector Additivity</a></li>
</ul>
<h2 id="some-backstory"><a href="#some-backstory" aria-hidden="true"><span class="icon icon-link"></span></a>Some backstory</h2>
<p>I got interested in word2vec and related techniques for my general interest in machine learning and for my general appreciations of:</p>
<ul>
<li>matrix factorization,</li>
<li>pointwise mutual information,</li>
<li>conceptual metaphors,</li>
<li>simple techniques mimicking human cognition.</li>
</ul>
<p>I had an motivation to learn more on the subject as I was tutoring Julia Bazińska during a two-week summer internship at <a href="http://www.delab.uw.edu.pl/" target="_blank" rel="nofollow noopener noreferrer">DELab, University of Warsaw</a>, supported by the Polish Children's Fund. See also my blog posts:</p>
<ul>
<li><a href="http://crastina.se/gifted-children-in-poland-by-piotr-migdal/" target="_blank" rel="nofollow noopener noreferrer">Helping exceptionally gifted children in Poland</a> - on the Polish Children's Fund</li>
<li><a href="http://p.migdal.pl/2016/02/09/d3js-icm-kfnrd.html" target="_blank" rel="nofollow noopener noreferrer">D3.js workshop at ICM for KFnrD</a> in Jan 2016, where it all started</li>
</ul>
<h2 id="thanks"><a href="#thanks" aria-hidden="true"><span class="icon icon-link"></span></a>Thanks</h2>
<p>This draft benefited from feedback from <a href="https://github.com/grzegorz225/" target="_blank" rel="nofollow noopener noreferrer">Grzegorz Uriasz</a> (what's simple and what isn't), <a href="http://goodsexlifestyle.com/" target="_blank" rel="nofollow noopener noreferrer">Sarah Martin</a> (readability and grammar remarks). I want to especially thank <a href="https://levyomer.wordpress.com/" target="_blank" rel="nofollow noopener noreferrer">Levy Omer</a> for pointing to weak points (and shady assumptions) of word vector arithmetics.</p>
<h2 id="edit"><a href="#edit" aria-hidden="true"><span class="icon icon-link"></span></a>EDIT</h2>
<p>It got some popularity, including ~20k views in the first day, and being tweeted by <a href="https://twitter.com/stanfordnlp/status/818881718077661184" target="_blank" rel="nofollow noopener noreferrer">the authors of GloVe at Stanford</a> and <a href="https://twitter.com/kaggle/status/819258895235424258" target="_blank" rel="nofollow noopener noreferrer">Kaggle</a>.</p>
<p>Though, what I like the most is to see how people interact with it:</p>
<ul>
<li><a href="https://twitter.com/cesifoti/status/818672972743450624" target="_blank" rel="nofollow noopener noreferrer">artistic-scientific impulsive-analytical</a> by Cesar Hidalgo from MIT Media Lab</li>
<li><a href="http://imgur.com/3FzX81i" target="_blank" rel="nofollow noopener noreferrer">good-evil unlawful-lawful and AD&#x26;D classes</a> from HN comment thread :)
$$</li>
</ul>
</div></div>
    <script src="/assets/js/app.5b89ac61.js" defer></script><script src="/assets/js/page--src--templates--blog-post-vue.bf85aef2.js" defer></script>
  </body>
</html>
